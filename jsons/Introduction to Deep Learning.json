{"chunks": [{"title": "Introduction to Deep Learning", "start": 0.0, "end": 15.780000000000001, "text": " So why are we all here?"}, {"title": "Introduction to Deep Learning", "start": 15.780000000000001, "end": 21.54, "text": " Deep learning has clearly been exploding in society."}, {"title": "Introduction to Deep Learning", "start": 21.54, "end": 26.38, "text": " Machine learning generally is something that when I started studying it about 13 years"}, {"title": "Introduction to Deep Learning", "start": 26.38, "end": 28.24, "text": " ago didn't work."}, {"title": "Introduction to Deep Learning", "start": 28.24, "end": 30.5, "text": " And now it works."}, {"title": "Introduction to Deep Learning", "start": 30.5, "end": 34.98, "text": " So how many of you here in this room used AI in the last week?"}, {"title": "Introduction to Deep Learning", "start": 34.98, "end": 36.78, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 36.78, "end": 38.16, "text": " Almost everybody."}, {"title": "Introduction to Deep Learning", "start": 38.16, "end": 39.84, "text": " Probably everybody."}, {"title": "Introduction to Deep Learning", "start": 39.84, "end": 50.019999999999996, "text": " And we're seeing massive advances in everything from AI-assisted text generation to 3D reconstruction,"}, {"title": "Introduction to Deep Learning", "start": 50.02, "end": 59.86, "text": " things like Nerf, things like AlphaGo, generating images, helping with writing code, playing"}, {"title": "Introduction to Deep Learning", "start": 59.86, "end": 61.02, "text": " games."}, {"title": "Introduction to Deep Learning", "start": 61.02, "end": 65.46000000000001, "text": " This has really started to touch almost every dimension of society, and it's definitely"}, {"title": "Introduction to Deep Learning", "start": 65.46000000000001, "end": 68.54, "text": " very exciting to see."}, {"title": "Introduction to Deep Learning", "start": 68.54, "end": 72.98, "text": " And I think that hopefully all of us will learn a lot more about what that kind of means"}, {"title": "Introduction to Deep Learning", "start": 72.98, "end": 74.7, "text": " in this course."}, {"title": "Introduction to Deep Learning", "start": 74.7, "end": 76.98, "text": " So what is deep learning?"}, {"title": "Introduction to Deep Learning", "start": 76.98, "end": 84.7, "text": " First, I would argue that one necessary component of deep learning is the idea of neural networks."}, {"title": "Introduction to Deep Learning", "start": 84.7, "end": 89.98, "text": " And these are a class of machine learning architectures that basically use stacks of"}, {"title": "Introduction to Deep Learning", "start": 89.98, "end": 95.98, "text": " linear transformations interleaved with pointwise nonlinearities."}, {"title": "Introduction to Deep Learning", "start": 95.98, "end": 101.22, "text": " And these have really been a building block for a lot of the progress that we saw on the"}, {"title": "Introduction to Deep Learning", "start": 101.22, "end": 103.26, "text": " last slide."}, {"title": "Introduction to Deep Learning", "start": 103.26, "end": 106.9, "text": " But the other really important dimension of deep learning is the idea of differential"}, {"title": "Introduction to Deep Learning", "start": 106.9, "end": 108.42, "text": " programming."}, {"title": "Introduction to Deep Learning", "start": 108.42, "end": 113.86, "text": " And this is essentially a programming paradigm where we parameterize parts of the program"}, {"title": "Introduction to Deep Learning", "start": 113.86, "end": 118.86, "text": " and then let gradient-based optimization tune the parameters to basically optimize that"}, {"title": "Introduction to Deep Learning", "start": 118.86, "end": 124.38000000000001, "text": " program or find some sort of at least local optimal for that program."}, {"title": "Introduction to Deep Learning", "start": 124.38000000000001, "end": 129.26, "text": " And these two things together is what we posit make up deep learning, and we're really going"}, {"title": "Introduction to Deep Learning", "start": 129.26, "end": 132.18, "text": " to dig into both of them in this course."}, {"title": "Introduction to Deep Learning", "start": 132.18, "end": 137.9, "text": " So our philosophy for this is that breakthroughs in deep learning have been driven by a mixture"}, {"title": "Introduction to Deep Learning", "start": 137.9, "end": 143.02, "text": " of theory and practice, and both dimensions are really vital for future progress in the"}, {"title": "Introduction to Deep Learning", "start": 143.02, "end": 144.86, "text": " field."}, {"title": "Introduction to Deep Learning", "start": 144.86, "end": 150.06, "text": " And so along those lines, this course should hopefully provide both a theoretical grounding"}, {"title": "Introduction to Deep Learning", "start": 150.06, "end": 156.66, "text": " in important deep learning building blocks as well as give you practice implementing,"}, {"title": "Introduction to Deep Learning", "start": 156.66, "end": 159.26000000000002, "text": " understanding, and using those blocks."}, {"title": "Introduction to Deep Learning", "start": 160.26, "end": 164.62, "text": " OK, there's still a lot of people in the back who are packed in."}, {"title": "Introduction to Deep Learning", "start": 164.62, "end": 167.7, "text": " I don't know if there's any way to come further forward."}, {"title": "Introduction to Deep Learning", "start": 167.7, "end": 171.34, "text": " If not, we'll just do our best."}, {"title": "Introduction to Deep Learning", "start": 171.34, "end": 176.34, "text": " So the class's coursework is going to be 65% problem sets."}, {"title": "Introduction to Deep Learning", "start": 176.34, "end": 177.42, "text": " You'll have five p-sets."}, {"title": "Introduction to Deep Learning", "start": 177.42, "end": 180.14, "text": " Each will be about one to two weeks long."}, {"title": "Introduction to Deep Learning", "start": 180.14, "end": 183.26, "text": " And then there will be some combination of pen and paper, or more realistically, maybe"}, {"title": "Introduction to Deep Learning", "start": 183.26, "end": 187.6, "text": " overleaf, as well as code that you'll need to do and submit."}, {"title": "Introduction to Deep Learning", "start": 187.64, "end": 190.35999999999999, "text": " And then 35% of the grade will be a final project."}, {"title": "Introduction to Deep Learning", "start": 190.35999999999999, "end": 193.96, "text": " And this is a research project that's focused on deeper understanding of some of the topics"}, {"title": "Introduction to Deep Learning", "start": 193.96, "end": 196.24, "text": " that we cover in the course."}, {"title": "Introduction to Deep Learning", "start": 196.24, "end": 198.88, "text": " You'll be asked to propose something for that project."}, {"title": "Introduction to Deep Learning", "start": 198.88, "end": 201.92, "text": " So it'll be an initial project proposal."}, {"title": "Introduction to Deep Learning", "start": 201.92, "end": 206.44, "text": " And then the final project will be a blog post that's going to demonstrate novel experimentation"}, {"title": "Introduction to Deep Learning", "start": 206.44, "end": 207.92, "text": " and visualization."}, {"title": "Introduction to Deep Learning", "start": 207.92, "end": 212.24, "text": " And the hope here is that, I don't know, many of you, how many of you are machine learning"}, {"title": "Introduction to Deep Learning", "start": 212.24, "end": 216.42, "text": " researchers who have already written a machine learning paper, is first author or anywhere"}, {"title": "Introduction to Deep Learning", "start": 216.45999999999998, "end": 217.45999999999998, "text": " on the author list?"}, {"title": "Introduction to Deep Learning", "start": 217.45999999999998, "end": 219.89999999999998, "text": " OK, so some."}, {"title": "Introduction to Deep Learning", "start": 219.89999999999998, "end": 223.1, "text": " These days, when you write a machine learning paper, more often than not, you also have"}, {"title": "Introduction to Deep Learning", "start": 223.1, "end": 225.54, "text": " to write a blog post about that paper."}, {"title": "Introduction to Deep Learning", "start": 225.54, "end": 231.22, "text": " And that's intended to be catchy, make people understand the material, but really get them"}, {"title": "Introduction to Deep Learning", "start": 231.22, "end": 232.22, "text": " engaged."}, {"title": "Introduction to Deep Learning", "start": 232.22, "end": 237.32, "text": " And so one of the reasons that we have this as the format of our final project is to reflect"}, {"title": "Introduction to Deep Learning", "start": 237.32, "end": 241.29999999999998, "text": " on the fact that this is the reality we live in today, that in machine learning research,"}, {"title": "Introduction to Deep Learning", "start": 241.3, "end": 247.38000000000002, "text": " being able to write a blog-style description of the work that you're doing and not reduce"}, {"title": "Introduction to Deep Learning", "start": 247.38000000000002, "end": 252.9, "text": " any of the technical complexity or the information that's given across, but just do this in this"}, {"title": "Introduction to Deep Learning", "start": 252.9, "end": 257.78000000000003, "text": " slightly more visual and maybe even interactive format, is a really valuable skill and one"}, {"title": "Introduction to Deep Learning", "start": 257.78000000000003, "end": 261.36, "text": " that we hope this can then help us think about."}, {"title": "Introduction to Deep Learning", "start": 261.36, "end": 266.22, "text": " And we'll allow groups, but only up to two."}, {"title": "Introduction to Deep Learning", "start": 266.22, "end": 270.38, "text": " And another important component, we are not able to provide compute."}, {"title": "Introduction to Deep Learning", "start": 270.46, "end": 274.7, "text": " We may be able to provide some very limited compute, but that's TBD."}, {"title": "Introduction to Deep Learning", "start": 274.7, "end": 279.7, "text": " So do not plan your final project to be something where you need to get state of the art by"}, {"title": "Introduction to Deep Learning", "start": 279.7, "end": 285.1, "text": " crunching a huge architecture on a bunch of huge data sets, because it's just not going"}, {"title": "Introduction to Deep Learning", "start": 285.1, "end": 286.1, "text": " to work."}, {"title": "Introduction to Deep Learning", "start": 286.1, "end": 290.78, "text": " You're not going to be able to out-compete open AI on this research project."}, {"title": "Introduction to Deep Learning", "start": 290.78, "end": 294.98, "text": " So this does not mean you can't do good research."}, {"title": "Introduction to Deep Learning", "start": 294.98, "end": 296.7, "text": " And it's important to get creative here."}, {"title": "Introduction to Deep Learning", "start": 297.02, "end": 302.26, "text": " I think it's actually a really valuable skill, particularly as certain types of large-scale"}, {"title": "Introduction to Deep Learning", "start": 302.26, "end": 307.06, "text": " deep learning become more and more centralized, to think about how you can still do impactful"}, {"title": "Introduction to Deep Learning", "start": 307.06, "end": 312.34, "text": " machine learning research in a way that is not just bigger is better."}, {"title": "Introduction to Deep Learning", "start": 312.34, "end": 313.98, "text": " All right."}, {"title": "Introduction to Deep Learning", "start": 313.98, "end": 316.94, "text": " Any questions about coursework or logistics?"}, {"title": "Introduction to Deep Learning", "start": 316.94, "end": 317.94, "text": " Yeah?"}, {"title": "Introduction to Deep Learning", "start": 317.94, "end": 325.3, "text": " Is the final project expected to be a mix of practical and theoretical research?"}, {"title": "Introduction to Deep Learning", "start": 325.3, "end": 329.66, "text": " So the question was, is the final project expected to be a mix of practical and theoretical"}, {"title": "Introduction to Deep Learning", "start": 329.66, "end": 330.78000000000003, "text": " research?"}, {"title": "Introduction to Deep Learning", "start": 330.78000000000003, "end": 332.38, "text": " I think it depends."}, {"title": "Introduction to Deep Learning", "start": 332.38, "end": 335.78000000000003, "text": " It depends on what you want to propose as your final project."}, {"title": "Introduction to Deep Learning", "start": 335.78000000000003, "end": 337.58000000000004, "text": " But I think we're pretty open."}, {"title": "Introduction to Deep Learning", "start": 337.58000000000004, "end": 339.82, "text": " What we want is for them to be innovative."}, {"title": "Introduction to Deep Learning", "start": 339.82, "end": 344.06, "text": " But that innovation can be in an applied sense, or it can be in a more theoretical sense."}, {"title": "Introduction to Deep Learning", "start": 344.06, "end": 345.06, "text": " OK."}, {"title": "Introduction to Deep Learning", "start": 345.06, "end": 349.62, "text": " So I'm going to do a really high-level overview of the schedule, just to give you a rough"}, {"title": "Introduction to Deep Learning", "start": 349.62, "end": 350.82, "text": " sense."}, {"title": "Introduction to Deep Learning", "start": 350.82, "end": 356.06, "text": " So today, we're really covering basic intro stuff, course overview, a rough introduction"}, {"title": "Introduction to Deep Learning", "start": 356.06, "end": 360.18, "text": " to what neural networks are and basic building blocks, and particularly in the context of"}, {"title": "Introduction to Deep Learning", "start": 360.18, "end": 364.18, "text": " signposting what we expect you've seen before, because this is not an intro to deep learning"}, {"title": "Introduction to Deep Learning", "start": 364.18, "end": 365.18, "text": " class."}, {"title": "Introduction to Deep Learning", "start": 365.18, "end": 368.94, "text": " This is advanced graduate-level deep learning."}, {"title": "Introduction to Deep Learning", "start": 368.94, "end": 375.7, "text": " And then we're going to go into how you actually train a neural net, then approximation theory,"}, {"title": "Introduction to Deep Learning", "start": 375.7, "end": 380.14, "text": " some different architectures, things like grids, graphs."}, {"title": "Introduction to Deep Learning", "start": 380.46, "end": 384.14, "text": " Then we'll hear from Jeremy about scaling rules for optimization."}, {"title": "Introduction to Deep Learning", "start": 384.14, "end": 389.21999999999997, "text": " And then we'll look at generalization theory, more architectures, getting into transformers."}, {"title": "Introduction to Deep Learning", "start": 389.21999999999997, "end": 393.26, "text": " We'll do a really fun lecture by Phil called The Hacker's Guide to Deep Learning that I"}, {"title": "Introduction to Deep Learning", "start": 393.26, "end": 395.64, "text": " think is quite useful."}, {"title": "Introduction to Deep Learning", "start": 395.64, "end": 397.7, "text": " And then we'll talk about memory."}, {"title": "Introduction to Deep Learning", "start": 397.7, "end": 400.06, "text": " And then we'll get into representation learning."}, {"title": "Introduction to Deep Learning", "start": 400.06, "end": 403.94, "text": " So there, we're going to look at reconstruction-based, and then similarity-based, and then theory"}, {"title": "Introduction to Deep Learning", "start": 403.94, "end": 405.78, "text": " of representation learning."}, {"title": "Introduction to Deep Learning", "start": 405.78, "end": 410.9, "text": " Then we'll have a little bit of a series on generative models, the basics, how representation"}, {"title": "Introduction to Deep Learning", "start": 410.9, "end": 415.85999999999996, "text": " learning interacts with generative modeling, thinking about conditional models."}, {"title": "Introduction to Deep Learning", "start": 415.85999999999996, "end": 419.97999999999996, "text": " Then we'll talk about generalization, and particularly out-of-distribution generalization."}, {"title": "Introduction to Deep Learning", "start": 419.97999999999996, "end": 424.21999999999997, "text": " We'll touch on transfer learning, both from a models and a data perspective."}, {"title": "Introduction to Deep Learning", "start": 424.21999999999997, "end": 427.02, "text": " We'll hear about large language models."}, {"title": "Introduction to Deep Learning", "start": 427.02, "end": 431.21999999999997, "text": " We'll have some guest lectures towards the end of the semester, and those are TBD."}, {"title": "Introduction to Deep Learning", "start": 431.22, "end": 436.1, "text": " Then we'll hear about scaling laws, automatic gradient descent, and potentially the past"}, {"title": "Introduction to Deep Learning", "start": 436.1, "end": 439.46000000000004, "text": " and future of deep learning, though I think there's some discussion as to whether that's"}, {"title": "Introduction to Deep Learning", "start": 439.46000000000004, "end": 442.38000000000005, "text": " going to be a different lecture."}, {"title": "Introduction to Deep Learning", "start": 442.38000000000005, "end": 448.14000000000004, "text": " And then I think, importantly, right towards the end, we will have class-wide project office"}, {"title": "Introduction to Deep Learning", "start": 448.14000000000004, "end": 452.18, "text": " hours where we'll have a bunch of the TAs and the instructors all come, and we can do"}, {"title": "Introduction to Deep Learning", "start": 452.18, "end": 456.34000000000003, "text": " open discussion around projects before they're due."}, {"title": "Introduction to Deep Learning", "start": 456.34000000000003, "end": 460.52000000000004, "text": " So the next thing is that we're going to host PyTorch tutorials."}, {"title": "Introduction to Deep Learning", "start": 460.52, "end": 465.03999999999996, "text": " This is intended for if you're not familiar with PyTorch and you think you need a refresher."}, {"title": "Introduction to Deep Learning", "start": 465.03999999999996, "end": 469.4, "text": " And we strongly recommend that if this is the case, or if you even think it might be"}, {"title": "Introduction to Deep Learning", "start": 469.4, "end": 473.2, "text": " the case, that you attend one of the two PyTorch tutorials we're going to offer next week."}, {"title": "Introduction to Deep Learning", "start": 473.2, "end": 476.44, "text": " So the question is if PyTorch is a requirement, are you allowed to use any other frameworks?"}, {"title": "Introduction to Deep Learning", "start": 476.44, "end": 480.18, "text": " So you're probably welcome to use any other frameworks in your final project, but some"}, {"title": "Introduction to Deep Learning", "start": 480.18, "end": 485.03999999999996, "text": " of the problem sets specifically will have code that's already built into PyTorch, and"}, {"title": "Introduction to Deep Learning", "start": 485.03999999999996, "end": 487.2, "text": " you're filling parts of that in."}, {"title": "Introduction to Deep Learning", "start": 487.2, "end": 491.06, "text": " So unless you want to rewrite all of it in JAX or something, which I mean, more power"}, {"title": "Introduction to Deep Learning", "start": 491.06, "end": 496.2, "text": " to you, I would recommend that you definitely make sure that you are familiar with PyTorch."}, {"title": "Introduction to Deep Learning", "start": 496.2, "end": 497.2, "text": " OK."}, {"title": "Introduction to Deep Learning", "start": 497.2, "end": 498.2, "text": " Cool."}, {"title": "Introduction to Deep Learning", "start": 498.2, "end": 504.0, "text": " So I'm going to talk about course policies, and this stuff is important, and it's also"}, {"title": "Introduction to Deep Learning", "start": 504.0, "end": 505.98, "text": " all on the web page."}, {"title": "Introduction to Deep Learning", "start": 505.98, "end": 509.91999999999996, "text": " But I think, essentially, what we want to touch on is what our collaboration policy"}, {"title": "Introduction to Deep Learning", "start": 509.91999999999996, "end": 511.4, "text": " is first."}, {"title": "Introduction to Deep Learning", "start": 511.4, "end": 515.16, "text": " So everyone is required to write an individual P-set."}, {"title": "Introduction to Deep Learning", "start": 515.16, "end": 518.92, "text": " So when you're actually working on your problem sets, do not work on something together and"}, {"title": "Introduction to Deep Learning", "start": 518.92, "end": 522.0, "text": " then both submit the exact same thing."}, {"title": "Introduction to Deep Learning", "start": 522.0, "end": 526.28, "text": " You can work on the actual content."}, {"title": "Introduction to Deep Learning", "start": 526.28, "end": 529.8199999999999, "text": " You can discuss that with peers, TAs, and instructors, but the problem sets you submit"}, {"title": "Introduction to Deep Learning", "start": 529.8199999999999, "end": 533.54, "text": " need to be your own independent and individual work."}, {"title": "Introduction to Deep Learning", "start": 533.54, "end": 535.8, "text": " And that also applies to the code that you write."}, {"title": "Introduction to Deep Learning", "start": 535.8, "end": 540.0799999999999, "text": " So you can discuss the code with others, but you need to each write your own code separately."}, {"title": "Introduction to Deep Learning", "start": 540.0799999999999, "end": 542.0799999999999, "text": " There was some confusion about this in the past."}, {"title": "Introduction to Deep Learning", "start": 542.24, "end": 546.08, "text": " Make sure that you're not taking the code someone else wrote, running it, and then submitting"}, {"title": "Introduction to Deep Learning", "start": 546.08, "end": 548.8000000000001, "text": " a P-set on it."}, {"title": "Introduction to Deep Learning", "start": 548.8000000000001, "end": 554.0, "text": " You should absolutely not copy or share complete solutions, and you also should not ask someone"}, {"title": "Introduction to Deep Learning", "start": 554.0, "end": 555.6800000000001, "text": " else, hey, this is my solution."}, {"title": "Introduction to Deep Learning", "start": 555.6800000000001, "end": 557.0400000000001, "text": " Is it right?"}, {"title": "Introduction to Deep Learning", "start": 557.0400000000001, "end": 561.5600000000001, "text": " And you shouldn't do that in person, and you also shouldn't ask, is this correct on Piazza"}, {"title": "Introduction to Deep Learning", "start": 561.5600000000001, "end": 562.5600000000001, "text": " or Canvas?"}, {"title": "Introduction to Deep Learning", "start": 562.5600000000001, "end": 566.88, "text": " So, again, the idea is it's your independent work, your independent thought."}, {"title": "Introduction to Deep Learning", "start": 566.88, "end": 570.48, "text": " And I think it matters more that you've gone through that process independently than actually"}, {"title": "Introduction to Deep Learning", "start": 570.64, "end": 575.2, "text": " getting it correct, because you'll learn from the feedback on your P-sets better if you"}, {"title": "Introduction to Deep Learning", "start": 575.2, "end": 578.88, "text": " submit something that you sort of came up with yourself and get feedback on it than"}, {"title": "Introduction to Deep Learning", "start": 578.88, "end": 581.52, "text": " if you copy somebody."}, {"title": "Introduction to Deep Learning", "start": 581.52, "end": 585.44, "text": " And then, if you work with anyone on the P-set other than TAs and instructors, we ask that"}, {"title": "Introduction to Deep Learning", "start": 585.44, "end": 586.9200000000001, "text": " you write their names at the top of the P-set."}, {"title": "Introduction to Deep Learning", "start": 586.9200000000001, "end": 589.12, "text": " So if you have a study group, you're all working together."}, {"title": "Introduction to Deep Learning", "start": 589.12, "end": 590.6, "text": " We don't care if this is 10 people."}, {"title": "Introduction to Deep Learning", "start": 590.6, "end": 595.32, "text": " We don't care if it's in this class 100 people, but we just want to know who was working together."}, {"title": "Introduction to Deep Learning", "start": 595.32, "end": 601.0400000000001, "text": " I'd be impressed if you managed to come up with 100 person collaborative group."}, {"title": "Introduction to Deep Learning", "start": 601.0400000000001, "end": 608.96, "text": " So AI assistants, we take the exact same policy about things like chat JPT or other AI assistants"}, {"title": "Introduction to Deep Learning", "start": 608.96, "end": 610.8000000000001, "text": " that we do with humans."}, {"title": "Introduction to Deep Learning", "start": 610.8000000000001, "end": 612.96, "text": " So it's a deep learning class."}, {"title": "Introduction to Deep Learning", "start": 612.96, "end": 616.48, "text": " You should be trying out the latest technology, right?"}, {"title": "Introduction to Deep Learning", "start": 616.48, "end": 623.0, "text": " The idea is not that we're teaching you to ignore what's happening, but we do want you"}, {"title": "Introduction to Deep Learning", "start": 623.0, "end": 625.6, "text": " to treat them like a human collaborator."}, {"title": "Introduction to Deep Learning", "start": 625.6, "end": 628.8, "text": " Don't ask them to answer the question for you, right?"}, {"title": "Introduction to Deep Learning", "start": 628.8, "end": 633.6, "text": " Just try to imagine from an ethics perspective that this is like a peer in the class."}, {"title": "Introduction to Deep Learning", "start": 633.6, "end": 635.56, "text": " So don't say, what is the solution to this?"}, {"title": "Introduction to Deep Learning", "start": 635.56, "end": 638.68, "text": " Don't say, write the code for me, because you wouldn't go to your friend and say, please"}, {"title": "Introduction to Deep Learning", "start": 638.68, "end": 641.52, "text": " write my code for me."}, {"title": "Introduction to Deep Learning", "start": 641.52, "end": 646.0, "text": " But you're super welcome to use them as a discussion partner."}, {"title": "Introduction to Deep Learning", "start": 646.0, "end": 649.16, "text": " Ask them questions that are contextual."}, {"title": "Introduction to Deep Learning", "start": 649.16, "end": 650.36, "text": " Kind of go back and forth."}, {"title": "Introduction to Deep Learning", "start": 650.36, "end": 651.36, "text": " That's all fine."}, {"title": "Introduction to Deep Learning", "start": 652.16, "end": 657.4, "text": " Don't treat AI collaboration as if they're not sort of a human collaborator."}, {"title": "Introduction to Deep Learning", "start": 657.4, "end": 658.72, "text": " Sort of think of it that way first."}, {"title": "Introduction to Deep Learning", "start": 658.72, "end": 662.5600000000001, "text": " Like, is this something I would ask my friend in the class?"}, {"title": "Introduction to Deep Learning", "start": 662.5600000000001, "end": 666.5600000000001, "text": " And then just like you can come to office hours and ask a human questions about the"}, {"title": "Introduction to Deep Learning", "start": 666.5600000000001, "end": 671.0, "text": " lecture material, clarifications about questions, tips for getting started, you're welcome to"}, {"title": "Introduction to Deep Learning", "start": 671.0, "end": 673.6800000000001, "text": " do the same with AI assistants."}, {"title": "Introduction to Deep Learning", "start": 673.6800000000001, "end": 677.52, "text": " Again, you're not allowed to ask an expert friend to do your homework for you."}, {"title": "Introduction to Deep Learning", "start": 677.52, "end": 681.24, "text": " Don't ask AI to do your homework for you."}, {"title": "Introduction to Deep Learning", "start": 681.24, "end": 684.76, "text": " If you're ever unclear, imagine AI as a human."}, {"title": "Introduction to Deep Learning", "start": 684.76, "end": 686.76, "text": " Apply that same norm."}, {"title": "Introduction to Deep Learning", "start": 686.76, "end": 691.6800000000001, "text": " If you work with any AI on a p-set, similarly to the fact that we're writing which humans"}, {"title": "Introduction to Deep Learning", "start": 691.6800000000001, "end": 696.48, "text": " we collaborated with, please write down which AI and how you used it."}, {"title": "Introduction to Deep Learning", "start": 696.48, "end": 702.0, "text": " And of course, this is a bit vague, but again, just kind of on our code."}, {"title": "Introduction to Deep Learning", "start": 702.0, "end": 705.36, "text": " Any questions about AI collaboration?"}, {"title": "Introduction to Deep Learning", "start": 705.36, "end": 706.76, "text": " Is that reasonably clear?"}, {"title": "Introduction to Deep Learning", "start": 706.76, "end": 707.76, "text": " Yeah?"}, {"title": "Introduction to Deep Learning", "start": 708.28, "end": 709.28, "text": " Yeah?"}, {"title": "Introduction to Deep Learning", "start": 709.28, "end": 711.28, "text": " I had a question on the last page."}, {"title": "Introduction to Deep Learning", "start": 711.28, "end": 715.28, "text": " So can we show our work to the TAs?"}, {"title": "Introduction to Deep Learning", "start": 715.28, "end": 719.08, "text": " Yes, you're very welcome to show your work to the TAs."}, {"title": "Introduction to Deep Learning", "start": 719.08, "end": 723.2, "text": " The TAs are not going to tell you how to answer the question though, but they will kind of"}, {"title": "Introduction to Deep Learning", "start": 723.2, "end": 726.68, "text": " work with you to help you hopefully figure it out on your own."}, {"title": "Introduction to Deep Learning", "start": 726.68, "end": 729.12, "text": " Why are we here?"}, {"title": "Introduction to Deep Learning", "start": 729.12, "end": 731.04, "text": " What's the goal?"}, {"title": "Introduction to Deep Learning", "start": 731.04, "end": 733.04, "text": " So what do we want to do?"}, {"title": "Introduction to Deep Learning", "start": 733.04, "end": 737.92, "text": " Why are we all interested in things like deep learning?"}, {"title": "Introduction to Deep Learning", "start": 737.92, "end": 744.56, "text": " One perspective is that we're interested in modeling complex phenomena in the real world."}, {"title": "Introduction to Deep Learning", "start": 744.56, "end": 745.9599999999999, "text": " What are complex phenomena?"}, {"title": "Introduction to Deep Learning", "start": 745.9599999999999, "end": 748.48, "text": " Well, you interact with them every day."}, {"title": "Introduction to Deep Learning", "start": 748.48, "end": 754.04, "text": " Things like natural language, images, or visual data in general, videos."}, {"title": "Introduction to Deep Learning", "start": 754.04, "end": 757.48, "text": " DNA, ecosystems are complex phenomena."}, {"title": "Introduction to Deep Learning", "start": 757.48, "end": 760.4399999999999, "text": " Climate change is a complex phenomena."}, {"title": "Introduction to Deep Learning", "start": 760.44, "end": 764.9200000000001, "text": " Why is it hard to model complex phenomena in the real world?"}, {"title": "Introduction to Deep Learning", "start": 764.9200000000001, "end": 768.6800000000001, "text": " They're complex."}, {"title": "Introduction to Deep Learning", "start": 768.6800000000001, "end": 773.7600000000001, "text": " And so maybe one existence proof for deep learning as a solution to modeling complex"}, {"title": "Introduction to Deep Learning", "start": 773.7600000000001, "end": 777.8800000000001, "text": " phenomena, possibly we could talk about the human brain."}, {"title": "Introduction to Deep Learning", "start": 777.8800000000001, "end": 781.4000000000001, "text": " All of you are here at MIT, which means you're pretty good at modeling complex phenomena"}, {"title": "Introduction to Deep Learning", "start": 781.4000000000001, "end": 785.84, "text": " in your brain, I'm guessing."}, {"title": "Introduction to Deep Learning", "start": 785.84, "end": 792.0, "text": " So first today, I'm going to just briefly go over how we got where we are today."}, {"title": "Introduction to Deep Learning", "start": 792.0, "end": 797.32, "text": " This is a brief history of the field of AI, deep learning in general."}, {"title": "Introduction to Deep Learning", "start": 797.32, "end": 801.36, "text": " And then I'm going to go over what we expect you have already seen before."}, {"title": "Introduction to Deep Learning", "start": 801.36, "end": 805.1, "text": " It's OK if you haven't seen these things before, but we would expect you then to go and brush"}, {"title": "Introduction to Deep Learning", "start": 805.1, "end": 806.64, "text": " up on them."}, {"title": "Introduction to Deep Learning", "start": 806.64, "end": 810.84, "text": " So if there's anything that I talk about and you're like, oh, I've never seen that, we"}, {"title": "Introduction to Deep Learning", "start": 810.84, "end": 811.84, "text": " expect that."}, {"title": "Introduction to Deep Learning", "start": 811.84, "end": 813.64, "text": " That's something you can go."}, {"title": "Introduction to Deep Learning", "start": 813.64, "end": 816.36, "text": " Look at some of the open coursework courses at MIT."}, {"title": "Introduction to Deep Learning", "start": 816.36, "end": 821.24, "text": " Go find lots of resources online and pick it up before you go into this class."}, {"title": "Introduction to Deep Learning", "start": 821.24, "end": 825.36, "text": " And then I'll also talk about what we actually cover in the class."}, {"title": "Introduction to Deep Learning", "start": 825.36, "end": 830.6999999999999, "text": " So a brief history of neural networks, and I think a fun way to go through this is looking"}, {"title": "Introduction to Deep Learning", "start": 830.6999999999999, "end": 835.1999999999999, "text": " at this on a plot of enthusiasm for neural networks over time."}, {"title": "Introduction to Deep Learning", "start": 835.2, "end": 843.32, "text": " So if we go way back to 1958, Rosenblatt introduced the perceptron in psychological"}, {"title": "Introduction to Deep Learning", "start": 843.32, "end": 844.32, "text": " review."}, {"title": "Introduction to Deep Learning", "start": 844.32, "end": 849.96, "text": " And the perceptron was the first neural network, arguably."}, {"title": "Introduction to Deep Learning", "start": 849.96, "end": 856.24, "text": " And what this was intending to do was recognize or categorize images."}, {"title": "Introduction to Deep Learning", "start": 856.24, "end": 862.6800000000001, "text": " And the idea was essentially that you could take some combination of representations,"}, {"title": "Introduction to Deep Learning", "start": 862.68, "end": 868.68, "text": " in this case, particularly pixels, sum them, put them through some non-linearity, and get"}, {"title": "Introduction to Deep Learning", "start": 868.68, "end": 871.0999999999999, "text": " out a categorization."}, {"title": "Introduction to Deep Learning", "start": 871.0999999999999, "end": 876.88, "text": " And really, we'll see throughout the course and even today, this idea still forms the"}, {"title": "Introduction to Deep Learning", "start": 876.88, "end": 880.76, "text": " building block for most of the deep learning that we do."}, {"title": "Introduction to Deep Learning", "start": 880.76, "end": 886.1999999999999, "text": " These perceptrons are components in many, many, many different types of capacities within"}, {"title": "Introduction to Deep Learning", "start": 886.1999999999999, "end": 888.18, "text": " things like Chat GPT."}, {"title": "Introduction to Deep Learning", "start": 888.18, "end": 892.8199999999999, "text": " When this paper came out back in 1958, people were incredibly enthusiastic."}, {"title": "Introduction to Deep Learning", "start": 892.8199999999999, "end": 896.0, "text": " It made waves."}, {"title": "Introduction to Deep Learning", "start": 896.0, "end": 904.2199999999999, "text": " And then in 1972, Minsky and Papert wrote a book called Perceptrons, Expanded Edition."}, {"title": "Introduction to Deep Learning", "start": 904.2199999999999, "end": 909.0999999999999, "text": " And this book takes a very critical lens to the idea that a perceptron could be seriously"}, {"title": "Introduction to Deep Learning", "start": 909.0999999999999, "end": 911.0999999999999, "text": " considered a model of the brain."}, {"title": "Introduction to Deep Learning", "start": 911.0999999999999, "end": 917.0999999999999, "text": " And it also carefully and mathematically describes and characterizes the limitations of what"}, {"title": "Introduction to Deep Learning", "start": 917.1, "end": 919.46, "text": " a perceptron can model."}, {"title": "Introduction to Deep Learning", "start": 919.46, "end": 925.3000000000001, "text": " And it did so, so carefully and so critically that all of a sudden, enthusiasm for machine"}, {"title": "Introduction to Deep Learning", "start": 925.3000000000001, "end": 927.82, "text": " learning really took a dip."}, {"title": "Introduction to Deep Learning", "start": 927.82, "end": 932.9, "text": " So in the 70s, we were not excited about neural networks."}, {"title": "Introduction to Deep Learning", "start": 932.9, "end": 939.1, "text": " But then in 1986, this book came out called Parallel Distributed Processing."}, {"title": "Introduction to Deep Learning", "start": 939.1, "end": 945.22, "text": " And this was fundamental because it introduces the concept of backpropagation."}, {"title": "Introduction to Deep Learning", "start": 945.22, "end": 950.62, "text": " And this was really the thing that enabled multi-layer perceptrons, so not just a single"}, {"title": "Introduction to Deep Learning", "start": 950.62, "end": 955.14, "text": " perceptron, but actually multiple perceptrons stacked on top of each other to actually be"}, {"title": "Introduction to Deep Learning", "start": 955.14, "end": 960.6600000000001, "text": " reasonably trained, reasonably fit, this concept of being able to backpropagate a gradient"}, {"title": "Introduction to Deep Learning", "start": 960.6600000000001, "end": 963.74, "text": " through multiple layers of perceptrons."}, {"title": "Introduction to Deep Learning", "start": 963.74, "end": 967.82, "text": " And what this enabled you to solve were things like the XOR problem."}, {"title": "Introduction to Deep Learning", "start": 967.82, "end": 972.5600000000001, "text": " For the first time, you could reasonably solve categorization problems that were not separable"}, {"title": "Introduction to Deep Learning", "start": 972.5600000000001, "end": 974.76, "text": " from a linear classifier."}, {"title": "Introduction to Deep Learning", "start": 975.3, "end": 977.08, "text": " They pointed to backpropagation as a breakthrough."}, {"title": "Introduction to Deep Learning", "start": 977.08, "end": 984.24, "text": " It let us actually handle things where you need multiple different boundaries and these"}, {"title": "Introduction to Deep Learning", "start": 984.24, "end": 989.4399999999999, "text": " non-linear boundaries to be drawn, which is only possible if you have multiple layers"}, {"title": "Introduction to Deep Learning", "start": 989.4399999999999, "end": 990.4399999999999, "text": " of a perceptron."}, {"title": "Introduction to Deep Learning", "start": 990.4399999999999, "end": 996.8, "text": " You'll never be able to solve the XOR problem with a single layer neural network."}, {"title": "Introduction to Deep Learning", "start": 996.8, "end": 998.96, "text": " And everyone got really excited again."}, {"title": "Introduction to Deep Learning", "start": 998.96, "end": 1003.8, "text": " So in the 80s, yet again, machine learning was something that people were pumped about,"}, {"title": "Introduction to Deep Learning", "start": 1003.8399999999999, "end": 1008.3599999999999, "text": " and particularly this deep learning neural network perspective on machine learning."}, {"title": "Introduction to Deep Learning", "start": 1008.3599999999999, "end": 1014.4399999999999, "text": " In 1998, Jan LeCun put out Convolutional Neural Networks."}, {"title": "Introduction to Deep Learning", "start": 1014.4399999999999, "end": 1016.88, "text": " There's an awesome demo that he has on his web page."}, {"title": "Introduction to Deep Learning", "start": 1016.88, "end": 1020.64, "text": " I'm sure many of you have heard of Jan LeCun."}, {"title": "Introduction to Deep Learning", "start": 1020.64, "end": 1027.94, "text": " And this seems like it would mean that everyone was really excited about neural networks again."}, {"title": "Introduction to Deep Learning", "start": 1027.94, "end": 1034.46, "text": " But if you go to NURiPS in 2000, this is the premier conference on machine learning."}, {"title": "Introduction to Deep Learning", "start": 1034.46, "end": 1041.6200000000001, "text": " It evolved from an interdisciplinary conference, which was on basically the science of the"}, {"title": "Introduction to Deep Learning", "start": 1041.6200000000001, "end": 1047.22, "text": " brain and machine learning, to specifically just a machine learning conference."}, {"title": "Introduction to Deep Learning", "start": 1047.22, "end": 1055.06, "text": " If you look at that conference in 2000, the most predictive words and papers on the title"}, {"title": "Introduction to Deep Learning", "start": 1055.1, "end": 1059.74, "text": " for acceptance were belief propagation and Gaussian."}, {"title": "Introduction to Deep Learning", "start": 1059.74, "end": 1067.1799999999998, "text": " And the title words most predictive of paper rejection were neural and network."}, {"title": "Introduction to Deep Learning", "start": 1067.1799999999998, "end": 1070.4199999999998, "text": " So this is what we called the AI winter."}, {"title": "Introduction to Deep Learning", "start": 1070.4199999999998, "end": 1074.02, "text": " And one of the reasons it was called the AI winter is because even though we had the theory,"}, {"title": "Introduction to Deep Learning", "start": 1074.02, "end": 1082.1399999999999, "text": " we had the building blocks, we didn't actually have the ability to train them efficiently."}, {"title": "Introduction to Deep Learning", "start": 1082.1399999999999, "end": 1084.76, "text": " We didn't have the right programming perspective."}, {"title": "Introduction to Deep Learning", "start": 1084.76, "end": 1087.84, "text": " And we didn't have the right hardware."}, {"title": "Introduction to Deep Learning", "start": 1087.84, "end": 1094.56, "text": " But then in 2012, Alex Kurchevsky and his co-authors published AlexNet."}, {"title": "Introduction to Deep Learning", "start": 1094.56, "end": 1100.24, "text": " And AlexNet was also another convolutional neural network."}, {"title": "Introduction to Deep Learning", "start": 1100.24, "end": 1104.72, "text": " But importantly, Alex Kurchevsky was a brilliant programmer."}, {"title": "Introduction to Deep Learning", "start": 1104.72, "end": 1109.36, "text": " And Alex Kurchevsky figured out how to program GPUs, graphics processing units, that have"}, {"title": "Introduction to Deep Learning", "start": 1109.36, "end": 1118.1599999999999, "text": " been developed to handle massive scale parallel multiplications for graphics, for video games,"}, {"title": "Introduction to Deep Learning", "start": 1118.1599999999999, "end": 1124.28, "text": " and for high resolution images."}, {"title": "Introduction to Deep Learning", "start": 1124.28, "end": 1128.84, "text": " But he figured out how to program those and repurpose that hardware for the training of"}, {"title": "Introduction to Deep Learning", "start": 1128.84, "end": 1130.6799999999998, "text": " neural networks."}, {"title": "Introduction to Deep Learning", "start": 1130.6799999999998, "end": 1135.84, "text": " And I can't tell you how much of a shot heard on the world this was in our field."}, {"title": "Introduction to Deep Learning", "start": 1135.84, "end": 1143.72, "text": " But this, for the first time, outperformed every single other possible method that was"}, {"title": "Introduction to Deep Learning", "start": 1143.72, "end": 1147.4399999999998, "text": " out there on ImageNet."}, {"title": "Introduction to Deep Learning", "start": 1147.4399999999998, "end": 1150.6, "text": " How many of you have heard of ImageNet?"}, {"title": "Introduction to Deep Learning", "start": 1150.6, "end": 1154.24, "text": " ImageNet was the first very large scale data set of its kind."}, {"title": "Introduction to Deep Learning", "start": 1154.24, "end": 1157.9199999999998, "text": " And I actually think that this is an important component."}, {"title": "Introduction to Deep Learning", "start": 1157.9199999999998, "end": 1160.36, "text": " What made machine learning start to work?"}, {"title": "Introduction to Deep Learning", "start": 1160.36, "end": 1166.24, "text": " And I would argue that, of course, it was the theory and the programming, the ability"}, {"title": "Introduction to Deep Learning", "start": 1166.24, "end": 1169.56, "text": " to actually sort of efficiently fit these things."}, {"title": "Introduction to Deep Learning", "start": 1169.56, "end": 1174.24, "text": " But the other really vital component was the data."}, {"title": "Introduction to Deep Learning", "start": 1174.24, "end": 1180.6799999999998, "text": " Machine learning does not work without large scale curated labeled data in many capacities"}, {"title": "Introduction to Deep Learning", "start": 1180.6799999999998, "end": 1181.6799999999998, "text": " still."}, {"title": "Introduction to Deep Learning", "start": 1181.6799999999998, "end": 1184.28, "text": " So we're getting better and better at self-supervised learning."}, {"title": "Introduction to Deep Learning", "start": 1184.28, "end": 1189.32, "text": " But so all of a sudden, we were back."}, {"title": "Introduction to Deep Learning", "start": 1189.32, "end": 1190.52, "text": " Everyone was enthusiastic again."}, {"title": "Introduction to Deep Learning", "start": 1190.52, "end": 1196.36, "text": " And actually, if you look, these were kind of hype cycles about 28 years, both of them."}, {"title": "Introduction to Deep Learning", "start": 1196.36, "end": 1201.2, "text": " And so the question is, where are we going to be in 2028?"}, {"title": "Introduction to Deep Learning", "start": 1201.2, "end": 1204.6, "text": " So it's 2024 now."}, {"title": "Introduction to Deep Learning", "start": 1204.6, "end": 1205.6, "text": " It's 2024."}, {"title": "Introduction to Deep Learning", "start": 1205.6, "end": 1208.1599999999999, "text": " So in four years, where are we going to be?"}, {"title": "Introduction to Deep Learning", "start": 1208.1599999999999, "end": 1211.72, "text": " So I mean, one argument would be, OK, we're in another hype cycle."}, {"title": "Introduction to Deep Learning", "start": 1211.72, "end": 1215.3999999999999, "text": " But I would actually argue that that's probably not true."}, {"title": "Introduction to Deep Learning", "start": 1215.4, "end": 1219.92, "text": " Another one would be that we're just way off the deep end in terms of enthusiasm."}, {"title": "Introduction to Deep Learning", "start": 1219.92, "end": 1224.0400000000002, "text": " And by 2028, we'll be out in the stratosphere."}, {"title": "Introduction to Deep Learning", "start": 1224.0400000000002, "end": 1227.0400000000002, "text": " Or possibly, we're doing something like this."}, {"title": "Introduction to Deep Learning", "start": 1227.0400000000002, "end": 1231.2800000000002, "text": " Maybe in 2028, we're going to hit some new sort of plane of enthusiasm."}, {"title": "Introduction to Deep Learning", "start": 1231.2800000000002, "end": 1233.1200000000001, "text": " And then we'll start to realize limitations."}, {"title": "Introduction to Deep Learning", "start": 1233.1200000000001, "end": 1234.1200000000001, "text": " And we'll oscillate again."}, {"title": "Introduction to Deep Learning", "start": 1234.1200000000001, "end": 1235.1200000000001, "text": " Yeah?"}, {"title": "Introduction to Deep Learning", "start": 1235.1200000000001, "end": 1242.8400000000001, "text": " So the question is, will AI take over society by 2028?"}, {"title": "Introduction to Deep Learning", "start": 1242.8400000000001, "end": 1244.8400000000001, "text": " My answer is, let's see."}, {"title": "Introduction to Deep Learning", "start": 1245.84, "end": 1248.04, "text": " So what's deep learning today?"}, {"title": "Introduction to Deep Learning", "start": 1248.04, "end": 1250.72, "text": " What are some of the components that make it up?"}, {"title": "Introduction to Deep Learning", "start": 1250.72, "end": 1255.28, "text": " First, I think a really important one is AutoGrad."}, {"title": "Introduction to Deep Learning", "start": 1255.28, "end": 1258.04, "text": " These are coding languages like PyTorch and TensorFlow."}, {"title": "Introduction to Deep Learning", "start": 1258.04, "end": 1263.84, "text": " They give us a really clever way to do things like implement the chain rule in software,"}, {"title": "Introduction to Deep Learning", "start": 1263.84, "end": 1266.84, "text": " making good use of available hardware."}, {"title": "Introduction to Deep Learning", "start": 1266.84, "end": 1272.1599999999999, "text": " And this has enabled us to do things like approximate the gradient of any function,"}, {"title": "Introduction to Deep Learning", "start": 1272.1599999999999, "end": 1274.4399999999998, "text": " even if it's an approximation."}, {"title": "Introduction to Deep Learning", "start": 1275.04, "end": 1277.04, "text": " So these are a really important component of deep learning."}, {"title": "Introduction to Deep Learning", "start": 1277.04, "end": 1281.1200000000001, "text": " And if you do anything with deep learning, I doubt that you're doing what Kurchevsky"}, {"title": "Introduction to Deep Learning", "start": 1281.1200000000001, "end": 1282.1200000000001, "text": " did."}, {"title": "Introduction to Deep Learning", "start": 1282.1200000000001, "end": 1283.3600000000001, "text": " I doubt that you're programming these things from scratch."}, {"title": "Introduction to Deep Learning", "start": 1283.3600000000001, "end": 1286.88, "text": " You're almost certainly working with existing programming languages that are built for machine"}, {"title": "Introduction to Deep Learning", "start": 1286.88, "end": 1288.8400000000001, "text": " learning."}, {"title": "Introduction to Deep Learning", "start": 1288.8400000000001, "end": 1292.8400000000001, "text": " It's billion-plus data point data sets, things like Lion."}, {"title": "Introduction to Deep Learning", "start": 1292.8400000000001, "end": 1296.24, "text": " So that scale of data that I talked about before, this is an incredibly important component"}, {"title": "Introduction to Deep Learning", "start": 1296.24, "end": 1298.16, "text": " of all of this."}, {"title": "Introduction to Deep Learning", "start": 1298.16, "end": 1301.52, "text": " Parallel training on thousands of GPUs."}, {"title": "Introduction to Deep Learning", "start": 1301.6, "end": 1305.56, "text": " Again, this is what we're looking at today."}, {"title": "Introduction to Deep Learning", "start": 1305.56, "end": 1310.6, "text": " Billion-plus parameter architectures, and this number just keeps increasing."}, {"title": "Introduction to Deep Learning", "start": 1310.6, "end": 1313.0, "text": " Million-plus dollar training costs."}, {"title": "Introduction to Deep Learning", "start": 1313.0, "end": 1318.12, "text": " So of course, it's not public knowledge how much it costs to train something like GPT-4."}, {"title": "Introduction to Deep Learning", "start": 1318.12, "end": 1323.96, "text": " But I would bet it's definitely in the many millions."}, {"title": "Introduction to Deep Learning", "start": 1323.96, "end": 1325.48, "text": " Shockingly good results."}, {"title": "Introduction to Deep Learning", "start": 1325.48, "end": 1326.48, "text": " I don't know about any of you."}, {"title": "Introduction to Deep Learning", "start": 1326.48, "end": 1329.04, "text": " I mean, I've been working in this field now for a while."}, {"title": "Introduction to Deep Learning", "start": 1330.04, "end": 1335.6399999999999, "text": " I think we've been seeing results recently that I didn't anticipate seeing in my lifetime"}, {"title": "Introduction to Deep Learning", "start": 1335.6399999999999, "end": 1338.52, "text": " from when I started, back when things didn't work."}, {"title": "Introduction to Deep Learning", "start": 1338.52, "end": 1342.6399999999999, "text": " So that's been both exciting to see and also sometimes, I think, for many of us, a bit"}, {"title": "Introduction to Deep Learning", "start": 1342.6399999999999, "end": 1343.6399999999999, "text": " overwhelming, right?"}, {"title": "Introduction to Deep Learning", "start": 1343.6399999999999, "end": 1347.6599999999999, "text": " Because you're like, oh gosh, the problems I thought were problems aren't problems anymore."}, {"title": "Introduction to Deep Learning", "start": 1347.6599999999999, "end": 1349.7, "text": " So what are the problems, right?"}, {"title": "Introduction to Deep Learning", "start": 1349.7, "end": 1353.24, "text": " Of course, I work in the world of..."}, {"title": "Introduction to Deep Learning", "start": 1353.24, "end": 1358.48, "text": " I work a lot on biodiversity loss at a global scale using machine learning."}, {"title": "Introduction to Deep Learning", "start": 1358.92, "end": 1361.92, "text": " And I would say that biodiversity loss is definitely still a problem, and one that machine"}, {"title": "Introduction to Deep Learning", "start": 1361.92, "end": 1368.56, "text": " learning is sometimes helping, but often also hurting, because the carbon costs of actually"}, {"title": "Introduction to Deep Learning", "start": 1368.56, "end": 1372.3600000000001, "text": " training these models is significant."}, {"title": "Introduction to Deep Learning", "start": 1372.3600000000001, "end": 1377.48, "text": " But all of these massive things, massive isn't necessary."}, {"title": "Introduction to Deep Learning", "start": 1377.48, "end": 1381.9, "text": " So things like stable diffusion are actually pretty lightweight and were incredibly impactful."}, {"title": "Introduction to Deep Learning", "start": 1381.9, "end": 1386.3600000000001, "text": " So things don't always need to be big to be good."}, {"title": "Introduction to Deep Learning", "start": 1386.4399999999998, "end": 1390.4799999999998, "text": " And then I think another really important component of deep learning today is the idea"}, {"title": "Introduction to Deep Learning", "start": 1390.4799999999998, "end": 1395.3999999999999, "text": " that it is primarily an open source community where we have what we call modular reuse,"}, {"title": "Introduction to Deep Learning", "start": 1395.3999999999999, "end": 1396.3999999999999, "text": " right?"}, {"title": "Introduction to Deep Learning", "start": 1396.3999999999999, "end": 1400.56, "text": " So people take weights that were trained by one person with one architecture, they might"}, {"title": "Introduction to Deep Learning", "start": 1400.56, "end": 1406.0, "text": " take that and use that entire thing as a module within another larger system that has many"}, {"title": "Introduction to Deep Learning", "start": 1406.0, "end": 1409.8, "text": " different components that are being pulled from different things."}, {"title": "Introduction to Deep Learning", "start": 1409.8, "end": 1413.8, "text": " Now I think that this open source perspective is really something that's driven a lot of"}, {"title": "Introduction to Deep Learning", "start": 1413.8, "end": 1417.6399999999999, "text": " progress in machine learning, but also one of the things that we have been seeing in"}, {"title": "Introduction to Deep Learning", "start": 1417.6399999999999, "end": 1422.84, "text": " the last few years is that things are less and less likely to be open source."}, {"title": "Introduction to Deep Learning", "start": 1422.84, "end": 1427.52, "text": " You might have an API, but you aren't necessarily going to get the weights and the architecture"}, {"title": "Introduction to Deep Learning", "start": 1427.52, "end": 1428.52, "text": " for something like GPT-4L."}, {"title": "Introduction to Deep Learning", "start": 1428.52, "end": 1435.84, "text": " So for the rest of the lecture, these are the signposting I'm going to use."}, {"title": "Introduction to Deep Learning", "start": 1435.84, "end": 1438.96, "text": " Looking back, this is what we expect you've seen before."}, {"title": "Introduction to Deep Learning", "start": 1438.96, "end": 1441.8799999999999, "text": " The green looking forward, that's what we're going to cover in this class."}, {"title": "Introduction to Deep Learning", "start": 1441.96, "end": 1442.96, "text": " All right."}, {"title": "Introduction to Deep Learning", "start": 1442.96, "end": 1445.24, "text": " Pretty straightforward."}, {"title": "Introduction to Deep Learning", "start": 1445.24, "end": 1450.1200000000001, "text": " I expect you have seen gradient descent before you come to this class."}, {"title": "Introduction to Deep Learning", "start": 1450.1200000000001, "end": 1458.5200000000002, "text": " So gradient descent essentially is the idea that we're trying to optimize some cost function,"}, {"title": "Introduction to Deep Learning", "start": 1458.5200000000002, "end": 1461.8000000000002, "text": " and the way that we're going to do that is we're finding some minimum over a set of all"}, {"title": "Introduction to Deep Learning", "start": 1461.8000000000002, "end": 1467.68, "text": " of our training data, some minimum set of parameters where then the loss is actually"}, {"title": "Introduction to Deep Learning", "start": 1467.68, "end": 1470.72, "text": " minimized for all of those different data points."}, {"title": "Introduction to Deep Learning", "start": 1470.72, "end": 1475.16, "text": " So what this actually looks like in practice, you're trying to find some theta star, which"}, {"title": "Introduction to Deep Learning", "start": 1475.16, "end": 1479.3600000000001, "text": " is like your optimal set of model parameters that minimize your cost function."}, {"title": "Introduction to Deep Learning", "start": 1479.3600000000001, "end": 1485.04, "text": " And so maybe you start somewhere random, and then you calculate your gradient."}, {"title": "Introduction to Deep Learning", "start": 1485.04, "end": 1490.2, "text": " Now you update the weights of that model in the direction of the gradient, and you step"}, {"title": "Introduction to Deep Learning", "start": 1490.2, "end": 1493.56, "text": " through, et cetera, et cetera."}, {"title": "Introduction to Deep Learning", "start": 1493.56, "end": 1499.6000000000001, "text": " And I think many of you have probably seen this before and will talk in an upcoming lecture,"}, {"title": "Introduction to Deep Learning", "start": 1499.6, "end": 1503.6799999999998, "text": " cover some of the different dimensions of things like stochastic gradient descent, et"}, {"title": "Introduction to Deep Learning", "start": 1503.6799999999998, "end": 1504.6799999999998, "text": " cetera."}, {"title": "Introduction to Deep Learning", "start": 1504.6799999999998, "end": 1509.7199999999998, "text": " But we're assuming that you've seen this idea before, probably an intro to ML lecture, for"}, {"title": "Introduction to Deep Learning", "start": 1509.7199999999998, "end": 1512.4399999999998, "text": " example."}, {"title": "Introduction to Deep Learning", "start": 1512.4399999999998, "end": 1516.7199999999998, "text": " Covering in this class, we'll go into a little bit more detail, back prop, and specifically"}, {"title": "Introduction to Deep Learning", "start": 1516.7199999999998, "end": 1519.6799999999998, "text": " this idea of differential programming."}, {"title": "Introduction to Deep Learning", "start": 1519.6799999999998, "end": 1524.24, "text": " So what does it actually look like to build programming languages that are structured"}, {"title": "Introduction to Deep Learning", "start": 1524.24, "end": 1529.76, "text": " around the idea of optimality and optimizing through gradient descent for different components"}, {"title": "Introduction to Deep Learning", "start": 1529.76, "end": 1532.22, "text": " of that architecture?"}, {"title": "Introduction to Deep Learning", "start": 1532.22, "end": 1536.8, "text": " And I think one of the things that's fun is thinking about what are you actually optimizing"}, {"title": "Introduction to Deep Learning", "start": 1536.8, "end": 1542.28, "text": " for and what are you pushing the gradients through to?"}, {"title": "Introduction to Deep Learning", "start": 1542.28, "end": 1551.2, "text": " So here, one iteration of gradient descent is essentially something like this."}, {"title": "Introduction to Deep Learning", "start": 1551.2, "end": 1555.2, "text": " And we have some learning rate."}, {"title": "Introduction to Deep Learning", "start": 1555.2, "end": 1558.96, "text": " And all of that, we're going to go into in a lot more detail next Tuesday, where we're"}, {"title": "Introduction to Deep Learning", "start": 1558.96, "end": 1563.72, "text": " going to be talking about where does this actually come through, how do we think about"}, {"title": "Introduction to Deep Learning", "start": 1563.72, "end": 1569.98, "text": " what some of these choices are, and how we actually build out the learning structure"}, {"title": "Introduction to Deep Learning", "start": 1569.98, "end": 1571.66, "text": " for back prop."}, {"title": "Introduction to Deep Learning", "start": 1571.66, "end": 1576.24, "text": " I do expect that you've seen things like multilayer perceptrons and nonlinearities, things like"}, {"title": "Introduction to Deep Learning", "start": 1576.24, "end": 1578.5800000000002, "text": " relues before."}, {"title": "Introduction to Deep Learning", "start": 1578.58, "end": 1583.82, "text": " So computation on neural network is often some form of vector in, vector out."}, {"title": "Introduction to Deep Learning", "start": 1583.82, "end": 1587.74, "text": " And this arrow that goes from vector in to vector out is a lot of what we're going to"}, {"title": "Introduction to Deep Learning", "start": 1587.74, "end": 1590.56, "text": " be talking about in this class."}, {"title": "Introduction to Deep Learning", "start": 1590.56, "end": 1593.6, "text": " What computation does that cover?"}, {"title": "Introduction to Deep Learning", "start": 1593.6, "end": 1595.1799999999998, "text": " How does that actually build out?"}, {"title": "Introduction to Deep Learning", "start": 1595.1799999999998, "end": 1599.82, "text": " So one cool thing about neural networks is that they often are reusing the same simple"}, {"title": "Introduction to Deep Learning", "start": 1599.82, "end": 1602.76, "text": " computational units over and over again."}, {"title": "Introduction to Deep Learning", "start": 1602.76, "end": 1607.6799999999998, "text": " So there's often a lot of the same kinds of computation that are being repeated and kind"}, {"title": "Introduction to Deep Learning", "start": 1607.78, "end": 1609.42, "text": " of reused."}, {"title": "Introduction to Deep Learning", "start": 1609.42, "end": 1614.28, "text": " And what these actually look like, these building blocks that are being used over and over again,"}, {"title": "Introduction to Deep Learning", "start": 1614.28, "end": 1617.74, "text": " they're often some combination of a linear layer."}, {"title": "Introduction to Deep Learning", "start": 1617.74, "end": 1622.78, "text": " So here, and we will be sticking with this type of notation for the rest of the class,"}, {"title": "Introduction to Deep Learning", "start": 1622.78, "end": 1627.5600000000002, "text": " what we call Z sub j, so some component of this output representation."}, {"title": "Introduction to Deep Learning", "start": 1627.5600000000002, "end": 1633.8400000000001, "text": " In that linear layer, that would be a sum over i of some weight times each of those"}, {"title": "Introduction to Deep Learning", "start": 1633.8400000000001, "end": 1636.44, "text": " input components."}, {"title": "Introduction to Deep Learning", "start": 1636.46, "end": 1638.98, "text": " Basically, pretty simple."}, {"title": "Introduction to Deep Learning", "start": 1638.98, "end": 1643.02, "text": " And that can also be written essentially this way, where you have some set of weights and"}, {"title": "Introduction to Deep Learning", "start": 1643.02, "end": 1645.18, "text": " then you also incorporate a bias term."}, {"title": "Introduction to Deep Learning", "start": 1645.18, "end": 1649.06, "text": " And this is a term that does not actually take in any of those input components."}, {"title": "Introduction to Deep Learning", "start": 1649.06, "end": 1651.78, "text": " It's just a bias."}, {"title": "Introduction to Deep Learning", "start": 1651.78, "end": 1655.8200000000002, "text": " And that can also be written in a vectorized notation, which makes everything much more"}, {"title": "Introduction to Deep Learning", "start": 1655.8200000000002, "end": 1656.8200000000002, "text": " simple."}, {"title": "Introduction to Deep Learning", "start": 1656.8200000000002, "end": 1663.9, "text": " So again, just with a focus on the notation, that output unit Z sub j is x, that input"}, {"title": "Introduction to Deep Learning", "start": 1663.92, "end": 1669.2800000000002, "text": " vector, transpose times W sub j, so the set of weights that correspond to that output"}, {"title": "Introduction to Deep Learning", "start": 1669.2800000000002, "end": 1672.8400000000001, "text": " component and that bias term."}, {"title": "Introduction to Deep Learning", "start": 1672.8400000000001, "end": 1676.8000000000002, "text": " And then what we consider theta, so that's all the parameters of the model, that's the"}, {"title": "Introduction to Deep Learning", "start": 1676.8000000000002, "end": 1681.3200000000002, "text": " set of all the weight terms and all the bias terms."}, {"title": "Introduction to Deep Learning", "start": 1681.3200000000002, "end": 1685.2, "text": " So what actually makes a neural net a neural net is the fact that it's not just linear"}, {"title": "Introduction to Deep Learning", "start": 1685.2, "end": 1686.8000000000002, "text": " combinations of inputs."}, {"title": "Introduction to Deep Learning", "start": 1686.8000000000002, "end": 1692.6000000000001, "text": " You also have some sort of nonlinear function that is mapping your outputs."}, {"title": "Introduction to Deep Learning", "start": 1692.8999999999999, "end": 1695.3, "text": " And that's something like G sub z."}, {"title": "Introduction to Deep Learning", "start": 1695.3, "end": 1698.78, "text": " And in this case, that's something like a pointwise nonlinearity."}, {"title": "Introduction to Deep Learning", "start": 1698.78, "end": 1703.32, "text": " So that's an important dimension, that these nonlinearities are usually pointwise."}, {"title": "Introduction to Deep Learning", "start": 1703.32, "end": 1706.1399999999999, "text": " So one possible nonlinearity would be this one, right?"}, {"title": "Introduction to Deep Learning", "start": 1706.1399999999999, "end": 1710.1399999999999, "text": " G sub z would be 1 if z is greater than 0 or 0 otherwise."}, {"title": "Introduction to Deep Learning", "start": 1710.1399999999999, "end": 1713.74, "text": " Is this a good choice of nonlinearity for a neural net?"}, {"title": "Introduction to Deep Learning", "start": 1713.74, "end": 1714.74, "text": " Why not?"}, {"title": "Introduction to Deep Learning", "start": 1714.74, "end": 1719.3, "text": " Yeah, so it's not differentiable, exactly."}, {"title": "Introduction to Deep Learning", "start": 1719.3999999999999, "end": 1722.8, "text": " Why is the fact that it's not differential not good?"}, {"title": "Introduction to Deep Learning", "start": 1722.8, "end": 1723.8, "text": " Yeah?"}, {"title": "Introduction to Deep Learning", "start": 1723.8, "end": 1727.56, "text": " It hinders back propagation."}, {"title": "Introduction to Deep Learning", "start": 1727.56, "end": 1729.28, "text": " It hinders back propagation."}, {"title": "Introduction to Deep Learning", "start": 1729.28, "end": 1730.28, "text": " Why?"}, {"title": "Introduction to Deep Learning", "start": 1730.28, "end": 1735.28, "text": " It makes the gradient descent harder."}, {"title": "Introduction to Deep Learning", "start": 1735.28, "end": 1737.76, "text": " Yeah, so the directionality, right?"}, {"title": "Introduction to Deep Learning", "start": 1737.76, "end": 1744.68, "text": " If you're anywhere on this graph, you don't know which way to go, essentially."}, {"title": "Introduction to Deep Learning", "start": 1744.68, "end": 1746.04, "text": " The gradient is 0."}, {"title": "Introduction to Deep Learning", "start": 1746.06, "end": 1748.58, "text": " So you can try to take a gradient descent step."}, {"title": "Introduction to Deep Learning", "start": 1748.58, "end": 1753.54, "text": " You're not going to move because the gradient is 0, exactly."}, {"title": "Introduction to Deep Learning", "start": 1753.54, "end": 1757.54, "text": " So this, roughly, is called a perceptron, right?"}, {"title": "Introduction to Deep Learning", "start": 1757.54, "end": 1762.62, "text": " This linear input plus a pointwise nonlinearity."}, {"title": "Introduction to Deep Learning", "start": 1762.62, "end": 1768.7, "text": " And you actually can, arguably, do linear classification with a perceptron, right?"}, {"title": "Introduction to Deep Learning", "start": 1768.7, "end": 1773.3799999999999, "text": " So if you take a really simple version of this, where your input is just two-dimensional,"}, {"title": "Introduction to Deep Learning", "start": 1773.4, "end": 1779.0800000000002, "text": " x1, x2, and you have some w1, w2 learned weight terms that you use to combine to get z, which"}, {"title": "Introduction to Deep Learning", "start": 1779.0800000000002, "end": 1784.8400000000001, "text": " is your hidden unit, and then you map that via a function to y, you then define z as"}, {"title": "Introduction to Deep Learning", "start": 1784.8400000000001, "end": 1786.8400000000001, "text": " x transpose w plus b."}, {"title": "Introduction to Deep Learning", "start": 1786.8400000000001, "end": 1791.44, "text": " And y is now some function g of z."}, {"title": "Introduction to Deep Learning", "start": 1791.44, "end": 1796.8400000000001, "text": " What this looks like for any set of values will be something like this, where you essentially"}, {"title": "Introduction to Deep Learning", "start": 1796.8400000000001, "end": 1799.3600000000001, "text": " have almost like a ramp, right?"}, {"title": "Introduction to Deep Learning", "start": 1799.3799999999999, "end": 1804.4599999999998, "text": " It's a clear and standard gradient across."}, {"title": "Introduction to Deep Learning", "start": 1804.4599999999998, "end": 1806.1799999999998, "text": " Why would it look like this?"}, {"title": "Introduction to Deep Learning", "start": 1806.1799999999998, "end": 1810.8999999999999, "text": " So why is it smooth and reasonably straight?"}, {"title": "Introduction to Deep Learning", "start": 1814.34, "end": 1818.02, "text": " I maybe didn't pose that question super clearly."}, {"title": "Introduction to Deep Learning", "start": 1818.02, "end": 1824.4599999999998, "text": " So essentially, the reason that mapping it looks like this is because it's a plane, right?"}, {"title": "Introduction to Deep Learning", "start": 1824.48, "end": 1831.96, "text": " So any sort of two-dimensional version of this is going to be a plane, right, with some"}, {"title": "Introduction to Deep Learning", "start": 1831.96, "end": 1834.0, "text": " orientation."}, {"title": "Introduction to Deep Learning", "start": 1834.0, "end": 1838.44, "text": " And then it's very straightforward, actually, to turn that into a linear classifier, because"}, {"title": "Introduction to Deep Learning", "start": 1838.44, "end": 1841.98, "text": " all you're doing is taking some threshold on that plane."}, {"title": "Introduction to Deep Learning", "start": 1841.98, "end": 1846.28, "text": " So even a single-layer neural network can perform linear classification, right?"}, {"title": "Introduction to Deep Learning", "start": 1846.28, "end": 1849.94, "text": " Because now this threshold, that's basically what that stepwise nonlinearity is giving"}, {"title": "Introduction to Deep Learning", "start": 1849.94, "end": 1850.94, "text": " you."}, {"title": "Introduction to Deep Learning", "start": 1850.94, "end": 1855.3600000000001, "text": " It's not something that is thresholded at some value."}, {"title": "Introduction to Deep Learning", "start": 1855.3600000000001, "end": 1859.2, "text": " And so even with something quite simple, you can do classification."}, {"title": "Introduction to Deep Learning", "start": 1859.2, "end": 1862.96, "text": " Now that's assuming that your data is linearly separable, right?"}, {"title": "Introduction to Deep Learning", "start": 1862.96, "end": 1867.3200000000002, "text": " So if you have training data like this, which is linearly separable, and now you want to"}, {"title": "Introduction to Deep Learning", "start": 1867.3200000000002, "end": 1874.48, "text": " find some optimal set of weights and biases that will minimize some loss term, where now,"}, {"title": "Introduction to Deep Learning", "start": 1874.48, "end": 1877.56, "text": " again, this loss is the difference between the true value."}, {"title": "Introduction to Deep Learning", "start": 1877.58, "end": 1883.62, "text": " So here, the true value is 1 or 0, and the functional mapping of the inputs to something"}, {"title": "Introduction to Deep Learning", "start": 1883.62, "end": 1884.62, "text": " like that value."}, {"title": "Introduction to Deep Learning", "start": 1884.62, "end": 1888.1799999999998, "text": " If you have something like this, this is what we'd call a bad fit, right?"}, {"title": "Introduction to Deep Learning", "start": 1888.1799999999998, "end": 1889.1799999999998, "text": " You have seven misclassifications."}, {"title": "Introduction to Deep Learning", "start": 1889.1799999999998, "end": 1893.58, "text": " You're actually doing, you're wrong about more things than you're right."}, {"title": "Introduction to Deep Learning", "start": 1893.58, "end": 1897.34, "text": " And all of those sort of red values, those are going to give you a high value in your"}, {"title": "Introduction to Deep Learning", "start": 1897.34, "end": 1899.1799999999998, "text": " loss function."}, {"title": "Introduction to Deep Learning", "start": 1899.1799999999998, "end": 1905.1399999999999, "text": " Or you could find something that's sort of like an OK fit, less misclassifications."}, {"title": "Introduction to Deep Learning", "start": 1905.3600000000001, "end": 1909.92, "text": " Maybe this is something that your model updates to after a single gradient step, right?"}, {"title": "Introduction to Deep Learning", "start": 1909.92, "end": 1915.72, "text": " Your decision service is linear, and now you're sort of fiddling around with what that classifier"}, {"title": "Introduction to Deep Learning", "start": 1915.72, "end": 1921.1200000000001, "text": " is, basically which values of weights and biases you need."}, {"title": "Introduction to Deep Learning", "start": 1921.1200000000001, "end": 1924.68, "text": " And then maybe after a few steps, you would find something that is a good fit, where you're"}, {"title": "Introduction to Deep Learning", "start": 1924.68, "end": 1927.3400000000001, "text": " not actually misclassifying anything."}, {"title": "Introduction to Deep Learning", "start": 1927.3400000000001, "end": 1932.3600000000001, "text": " So it is possible to do classification of linear things, linearly separable things,"}, {"title": "Introduction to Deep Learning", "start": 1932.3799999999999, "end": 1936.6999999999998, "text": " even with something like a perceptron that's doing these really simple thresholding-based"}, {"title": "Introduction to Deep Learning", "start": 1936.6999999999998, "end": 1939.58, "text": " nonlinearities."}, {"title": "Introduction to Deep Learning", "start": 1939.58, "end": 1945.08, "text": " But like we said, this actually would be really difficult to learn, right?"}, {"title": "Introduction to Deep Learning", "start": 1945.08, "end": 1950.0, "text": " So instead, maybe you would try something like a tanch."}, {"title": "Introduction to Deep Learning", "start": 1950.0, "end": 1954.4599999999998, "text": " So this is another pointwise nonlinearity."}, {"title": "Introduction to Deep Learning", "start": 1954.4599999999998, "end": 1958.78, "text": " And it was one of the sort of earliest ones that people looked at."}, {"title": "Introduction to Deep Learning", "start": 1958.78, "end": 1962.6399999999999, "text": " The things that are nice about this is it's bounded between minus 1 and 1, so none of"}, {"title": "Introduction to Deep Learning", "start": 1962.6399999999999, "end": 1965.04, "text": " the values are super huge."}, {"title": "Introduction to Deep Learning", "start": 1965.04, "end": 1968.0, "text": " And you get saturation of the gradient for super large or super small inputs."}, {"title": "Introduction to Deep Learning", "start": 1968.0, "end": 1971.68, "text": " You're not getting sort of explosions necessarily."}, {"title": "Introduction to Deep Learning", "start": 1971.68, "end": 1975.2, "text": " But also, if you start out somewhere that's really far from the center, you don't have"}, {"title": "Introduction to Deep Learning", "start": 1975.2, "end": 1976.8, "text": " a lot of training signal, right?"}, {"title": "Introduction to Deep Learning", "start": 1976.8, "end": 1979.92, "text": " Because we're getting close to 0 in our gradient."}, {"title": "Introduction to Deep Learning", "start": 1979.92, "end": 1984.8799999999999, "text": " And so those gradients do go to 0 as we go to infinity in either direction."}, {"title": "Introduction to Deep Learning", "start": 1984.8799999999999, "end": 1987.72, "text": " The outputs are centered at 0."}, {"title": "Introduction to Deep Learning", "start": 1987.74, "end": 1994.1000000000001, "text": " So the tanh of z can also be written as 2 times the sigmoid of 2z minus 1."}, {"title": "Introduction to Deep Learning", "start": 1994.1000000000001, "end": 1996.6200000000001, "text": " So let's look at what that sigmoid function looks like."}, {"title": "Introduction to Deep Learning", "start": 1996.6200000000001, "end": 2002.1000000000001, "text": " So in this case, this is a sigmoid, 1 over 1 plus e to the minus h."}, {"title": "Introduction to Deep Learning", "start": 2002.1000000000001, "end": 2005.58, "text": " And e to the minus..."}, {"title": "Introduction to Deep Learning", "start": 2005.58, "end": 2006.58, "text": " Notation is wrong."}, {"title": "Introduction to Deep Learning", "start": 2006.58, "end": 2008.66, "text": " It should be minus z."}, {"title": "Introduction to Deep Learning", "start": 2008.66, "end": 2011.3, "text": " This can be interpreted as the firing rate of a neuron."}, {"title": "Introduction to Deep Learning", "start": 2011.3, "end": 2014.54, "text": " It was kind of the one interpretation of it initially."}, {"title": "Introduction to Deep Learning", "start": 2014.56, "end": 2018.12, "text": " It's bounded between 0 and 1, so it's not ever negative."}, {"title": "Introduction to Deep Learning", "start": 2018.12, "end": 2023.84, "text": " Again, you have saturation of that gradient for super large or super small inputs."}, {"title": "Introduction to Deep Learning", "start": 2023.84, "end": 2026.6, "text": " And all those gradients go to 0."}, {"title": "Introduction to Deep Learning", "start": 2026.6, "end": 2028.96, "text": " The outputs themselves are centered around 0.5."}, {"title": "Introduction to Deep Learning", "start": 2028.96, "end": 2030.6399999999999, "text": " So it's kind of poor conditioning."}, {"title": "Introduction to Deep Learning", "start": 2030.6399999999999, "end": 2032.68, "text": " It's slightly biased."}, {"title": "Introduction to Deep Learning", "start": 2032.68, "end": 2035.32, "text": " In practice, we don't actually use this."}, {"title": "Introduction to Deep Learning", "start": 2035.32, "end": 2039.72, "text": " So instead, in practice, what we often use is something called a rectified linear unit"}, {"title": "Introduction to Deep Learning", "start": 2039.72, "end": 2041.28, "text": " or a ReLU."}, {"title": "Introduction to Deep Learning", "start": 2041.3, "end": 2045.8999999999999, "text": " And this is just the max of 0 and z."}, {"title": "Introduction to Deep Learning", "start": 2045.8999999999999, "end": 2048.86, "text": " And this is unbounded on the positive side, right?"}, {"title": "Introduction to Deep Learning", "start": 2048.86, "end": 2053.34, "text": " So it can go all the way to infinity, which can result in, for example, sometimes things"}, {"title": "Introduction to Deep Learning", "start": 2053.34, "end": 2056.7, "text": " like exploding gradients because the values can get very large."}, {"title": "Introduction to Deep Learning", "start": 2056.7, "end": 2058.74, "text": " But it's super efficient to implement."}, {"title": "Introduction to Deep Learning", "start": 2058.74, "end": 2063.42, "text": " Essentially, the derivatives are just that subfunction."}, {"title": "Introduction to Deep Learning", "start": 2063.42, "end": 2065.86, "text": " And it also seems to help speed up convergence."}, {"title": "Introduction to Deep Learning", "start": 2065.86, "end": 2071.02, "text": " So in that Krzyzewski paper, they showed that they got something like a 6x speedup using"}, {"title": "Introduction to Deep Learning", "start": 2071.7599999999998, "end": 2076.24, "text": " a ReLU over using something like a tanh in terms of converging that model and learning."}, {"title": "Introduction to Deep Learning", "start": 2076.24, "end": 2080.04, "text": " The drawback is that if you're strongly in the negative region, the unit's what we call"}, {"title": "Introduction to Deep Learning", "start": 2080.04, "end": 2081.68, "text": " dead."}, {"title": "Introduction to Deep Learning", "start": 2081.68, "end": 2082.7599999999998, "text": " There is no gradient."}, {"title": "Introduction to Deep Learning", "start": 2082.7599999999998, "end": 2088.32, "text": " And so you essentially have this gradient collapse where you can no longer learn from"}, {"title": "Introduction to Deep Learning", "start": 2088.32, "end": 2091.7599999999998, "text": " that sort of component of the vector."}, {"title": "Introduction to Deep Learning", "start": 2091.7599999999998, "end": 2092.7599999999998, "text": " But this is our default choice."}, {"title": "Introduction to Deep Learning", "start": 2092.7599999999998, "end": 2094.96, "text": " And it's widely used in current models."}, {"title": "Introduction to Deep Learning", "start": 2094.96, "end": 2097.12, "text": " There's lots of sort of slight tweaks to this."}, {"title": "Introduction to Deep Learning", "start": 2097.12, "end": 2098.52, "text": " But this is really, really common."}, {"title": "Introduction to Deep Learning", "start": 2099.02, "end": 2100.02, "text": " OK."}, {"title": "Introduction to Deep Learning", "start": 2100.02, "end": 2104.82, "text": " So then the last thing I want to talk about in this sort of section is stacking layers."}, {"title": "Introduction to Deep Learning", "start": 2104.82, "end": 2109.86, "text": " So we talked about sort of a single version of this where you have your linear projection."}, {"title": "Introduction to Deep Learning", "start": 2109.86, "end": 2111.9, "text": " And then you have a non-linearity."}, {"title": "Introduction to Deep Learning", "start": 2111.9, "end": 2115.1, "text": " But now you can take that and you can just stack another one on top of it."}, {"title": "Introduction to Deep Learning", "start": 2115.1, "end": 2119.02, "text": " So now both z and h are kind of hidden units."}, {"title": "Introduction to Deep Learning", "start": 2119.02, "end": 2120.62, "text": " They're somewhere in the middle of the model."}, {"title": "Introduction to Deep Learning", "start": 2120.62, "end": 2125.18, "text": " We don't necessarily, they're not seen at the inputs or the outputs."}, {"title": "Introduction to Deep Learning", "start": 2125.8399999999997, "end": 2129.2, "text": " And essentially, then you can also kind of remember that all of these things are kind"}, {"title": "Introduction to Deep Learning", "start": 2129.2, "end": 2133.16, "text": " of stacked on top of each other, almost in parallel as well, right?"}, {"title": "Introduction to Deep Learning", "start": 2133.16, "end": 2137.2799999999997, "text": " Because all of these inputs are mapping to the different components of the output vector."}, {"title": "Introduction to Deep Learning", "start": 2137.2799999999997, "end": 2140.64, "text": " And so you have weights for all of those different dimensions."}, {"title": "Introduction to Deep Learning", "start": 2140.64, "end": 2146.6, "text": " So this means that h, sort of our output of the first layer, is going to be g, that pointwise"}, {"title": "Introduction to Deep Learning", "start": 2146.6, "end": 2154.56, "text": " non-linearity, applied to now instead of vectorized w transpose x of j to give us sort of the"}, {"title": "Introduction to Deep Learning", "start": 2154.58, "end": 2158.9, "text": " individual jth component, you can actually just think of this now as a matrix multiplied"}, {"title": "Introduction to Deep Learning", "start": 2158.9, "end": 2159.9, "text": " by a vector, right?"}, {"title": "Introduction to Deep Learning", "start": 2159.9, "end": 2165.2999999999997, "text": " All of the weights for all of those different dimensions multiplied by the vector x plus"}, {"title": "Introduction to Deep Learning", "start": 2165.2999999999997, "end": 2167.98, "text": " the vector of all your biases b1."}, {"title": "Introduction to Deep Learning", "start": 2167.98, "end": 2170.9, "text": " So now you have this kind of cleaner notation."}, {"title": "Introduction to Deep Learning", "start": 2170.9, "end": 2176.9, "text": " And then now y is again g, that same pointwise non-linearity applied on top of your sort"}, {"title": "Introduction to Deep Learning", "start": 2176.9, "end": 2180.18, "text": " of outputs of your first thing."}, {"title": "Introduction to Deep Learning", "start": 2180.2, "end": 2185.68, "text": " And now with h, right, multiplied by that vector of all your second layer weights and"}, {"title": "Introduction to Deep Learning", "start": 2185.68, "end": 2188.24, "text": " added with your second layer bias term."}, {"title": "Introduction to Deep Learning", "start": 2188.24, "end": 2193.04, "text": " And so now your set theta, your model representation, is the set of all of the different weights"}, {"title": "Introduction to Deep Learning", "start": 2193.04, "end": 2196.16, "text": " from all the different layers as well as all the different biases in all the different"}, {"title": "Introduction to Deep Learning", "start": 2196.16, "end": 2197.16, "text": " layers."}, {"title": "Introduction to Deep Learning", "start": 2197.16, "end": 2198.16, "text": " Oh, there's a, thank you."}, {"title": "Introduction to Deep Learning", "start": 2198.16, "end": 2199.16, "text": " Oh yeah, thank you."}, {"title": "Introduction to Deep Learning", "start": 2199.16, "end": 2218.1, "text": " So you're asking, in practice, are there any best practices for choosing the non-linearity"}, {"title": "Introduction to Deep Learning", "start": 2218.1, "end": 2222.18, "text": " based on different types of data?"}, {"title": "Introduction to Deep Learning", "start": 2222.18, "end": 2226.74, "text": " I think in the literature, there's maybe sort of, you can kind of see it coming out."}, {"title": "Introduction to Deep Learning", "start": 2226.8399999999997, "end": 2232.3999999999996, "text": " But often, people really do use ReLU quite a lot."}, {"title": "Introduction to Deep Learning", "start": 2232.3999999999996, "end": 2237.74, "text": " But I mean, I wouldn't say that there's any sort of great general rules of thumb there."}, {"title": "Introduction to Deep Learning", "start": 2237.74, "end": 2248.08, "text": " And often, yeah, I would say there's like different things that people have converged"}, {"title": "Introduction to Deep Learning", "start": 2248.08, "end": 2251.4799999999996, "text": " upon based on experimental success, right?"}, {"title": "Introduction to Deep Learning", "start": 2251.4799999999996, "end": 2255.24, "text": " So you'll sort of see in the literature like, oh yeah, everything in this line of work on"}, {"title": "Introduction to Deep Learning", "start": 2255.24, "end": 2260.7, "text": " these data sets with these things, they're always using one or the other component."}, {"title": "Introduction to Deep Learning", "start": 2260.7, "end": 2262.58, "text": " And then people kind of build on that."}, {"title": "Introduction to Deep Learning", "start": 2262.58, "end": 2267.66, "text": " Sometimes that's called grad student gradient descent because you're essentially writing"}, {"title": "Introduction to Deep Learning", "start": 2267.66, "end": 2272.3399999999997, "text": " research papers and learning from those research papers what parameters in terms of things"}, {"title": "Introduction to Deep Learning", "start": 2272.3399999999997, "end": 2274.8199999999997, "text": " like your activation functions make sense."}, {"title": "Introduction to Deep Learning", "start": 2274.8199999999997, "end": 2279.58, "text": " But yeah, I don't know that I have great rules of thumb of like, oh, for this type of application,"}, {"title": "Introduction to Deep Learning", "start": 2279.58, "end": 2282.3399999999997, "text": " you should always use this or that."}, {"title": "Introduction to Deep Learning", "start": 2282.3399999999997, "end": 2283.3399999999997, "text": " Do you?"}, {"title": "Introduction to Deep Learning", "start": 2283.44, "end": 2285.44, "text": " Yeah, it's not a hard science."}, {"title": "Introduction to Deep Learning", "start": 2285.44, "end": 2286.44, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 2286.44, "end": 2290.44, "text": " It's usually about what makes the network more trainable rather than like trying to"}, {"title": "Introduction to Deep Learning", "start": 2290.44, "end": 2292.44, "text": " model a certain kind of data."}, {"title": "Introduction to Deep Learning", "start": 2292.44, "end": 2297.44, "text": " There are a couple of examples of like using a sinusoidal nonlinearity to pick out some"}, {"title": "Introduction to Deep Learning", "start": 2297.44, "end": 2298.44, "text": " free of people."}, {"title": "Introduction to Deep Learning", "start": 2298.44, "end": 2301.44, "text": " Like there are a few examples of it, but usually not."}, {"title": "Introduction to Deep Learning", "start": 2301.44, "end": 2306.44, "text": " And normally, if anyone does deep learning, has a great answer to that question."}, {"title": "Introduction to Deep Learning", "start": 2306.44, "end": 2307.44, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 2307.44, "end": 2309.44, "text": " So what Jeremy was saying is basically, it's not a hard science."}, {"title": "Introduction to Deep Learning", "start": 2309.54, "end": 2314.54, "text": " We don't have like sort of theoretical guarantees of like this is what you should do."}, {"title": "Introduction to Deep Learning", "start": 2314.54, "end": 2318.54, "text": " But in practice, there are, yeah, there's sort of counter examples to that."}, {"title": "Introduction to Deep Learning", "start": 2318.54, "end": 2322.44, "text": " So if you know, for example, that your features are sinusoidal, it maybe makes sense to try"}, {"title": "Introduction to Deep Learning", "start": 2322.44, "end": 2324.66, "text": " to pick out like Fourier representations."}, {"title": "Introduction to Deep Learning", "start": 2324.66, "end": 2328.54, "text": " You're working in the Fourier space, then maybe you should use a sinusoidal activation."}, {"title": "Introduction to Deep Learning", "start": 2328.54, "end": 2332.82, "text": " Anyway, but yes, I wish that we better understood it."}, {"title": "Introduction to Deep Learning", "start": 2332.82, "end": 2337.7400000000002, "text": " And that's, I think, why we need a lot more research on the theoretical side of like understanding,"}, {"title": "Introduction to Deep Learning", "start": 2338.04, "end": 2343.64, "text": " how do we actually choose these things appropriately instead of maybe learning everything from"}, {"title": "Introduction to Deep Learning", "start": 2343.64, "end": 2348.8399999999997, "text": " scratch or doing it experimentally, burning all those trees."}, {"title": "Introduction to Deep Learning", "start": 2348.8399999999997, "end": 2351.16, "text": " Cool."}, {"title": "Introduction to Deep Learning", "start": 2351.16, "end": 2354.4399999999996, "text": " So the parameters of our model are now all the different layers of weights and all the"}, {"title": "Introduction to Deep Learning", "start": 2354.4399999999996, "end": 2357.2, "text": " different layers of biases."}, {"title": "Introduction to Deep Learning", "start": 2357.2, "end": 2361.08, "text": " And so like we said, you can do linear classification with a perceptron."}, {"title": "Introduction to Deep Learning", "start": 2361.08, "end": 2366.2, "text": " But the fun thing is, oops, you can do nonlinear classification with a deep net, even if that"}, {"title": "Introduction to Deep Learning", "start": 2366.2, "end": 2369.48, "text": " deep net is just a two-layer perceptron."}, {"title": "Introduction to Deep Learning", "start": 2369.48, "end": 2374.46, "text": " So here, now we've built sort of a double-layer model still only in two dimensions."}, {"title": "Introduction to Deep Learning", "start": 2374.46, "end": 2380.3799999999997, "text": " So now we have mapped through essentially just this simple nonlinearity in between these"}, {"title": "Introduction to Deep Learning", "start": 2380.3799999999997, "end": 2381.3799999999997, "text": " two different layers."}, {"title": "Introduction to Deep Learning", "start": 2381.3799999999997, "end": 2388.62, "text": " Now, if we look at what this looks like, that first layer is giving us one kind of ramp."}, {"title": "Introduction to Deep Learning", "start": 2388.62, "end": 2392.54, "text": " The second layer gives us a different ramp because it's a different linear model."}, {"title": "Introduction to Deep Learning", "start": 2392.56, "end": 2397.4, "text": " And now if you do this nonlinear combination of them, essentially what you get out is you"}, {"title": "Introduction to Deep Learning", "start": 2397.4, "end": 2401.92, "text": " can actually map almost like it's almost like think about the intersection of one ramp with"}, {"title": "Introduction to Deep Learning", "start": 2401.92, "end": 2403.64, "text": " another ramp."}, {"title": "Introduction to Deep Learning", "start": 2403.64, "end": 2409.72, "text": " You get like a kind of triangular, almost like a pyramidal component that will be higher"}, {"title": "Introduction to Deep Learning", "start": 2409.72, "end": 2410.72, "text": " than everything else."}, {"title": "Introduction to Deep Learning", "start": 2410.72, "end": 2413.72, "text": " And so now you can still take a simple threshold of that."}, {"title": "Introduction to Deep Learning", "start": 2413.72, "end": 2416.4, "text": " And now you can get out a nonlinear classification."}, {"title": "Introduction to Deep Learning", "start": 2416.4, "end": 2419.32, "text": " And of course, you can expand this into many more dimensions."}, {"title": "Introduction to Deep Learning", "start": 2419.6600000000003, "end": 2424.1400000000003, "text": " This is, I think, some nice simple intuition building as to why you start getting nonlinear"}, {"title": "Introduction to Deep Learning", "start": 2424.1400000000003, "end": 2430.02, "text": " models when you're taking linear combinations but with these nonlinearities."}, {"title": "Introduction to Deep Learning", "start": 2430.02, "end": 2431.5, "text": " Cool."}, {"title": "Introduction to Deep Learning", "start": 2431.5, "end": 2435.6000000000004, "text": " So another, so we kind of expect that you've seen a lot of that before."}, {"title": "Introduction to Deep Learning", "start": 2435.6000000000004, "end": 2439.82, "text": " But moving forward, one of the things we are going to cover in this class is what we can"}, {"title": "Introduction to Deep Learning", "start": 2439.82, "end": 2441.28, "text": " approximate."}, {"title": "Introduction to Deep Learning", "start": 2441.28, "end": 2447.2200000000003, "text": " So what do we know about what is possible to approximate with deep learning?"}, {"title": "Introduction to Deep Learning", "start": 2447.24, "end": 2449.48, "text": " So one dimension of that is representational power."}, {"title": "Introduction to Deep Learning", "start": 2449.48, "end": 2452.16, "text": " So you have a single layer."}, {"title": "Introduction to Deep Learning", "start": 2452.16, "end": 2455.9599999999996, "text": " You can do any linear decision surface."}, {"title": "Introduction to Deep Learning", "start": 2455.9599999999996, "end": 2461.5, "text": " If you have two plus layers, in theory, you can actually represent any function."}, {"title": "Introduction to Deep Learning", "start": 2461.5, "end": 2465.3999999999996, "text": " And this is assuming, of course, that you have some non-trivial nonlinearity between"}, {"title": "Introduction to Deep Learning", "start": 2465.3999999999996, "end": 2466.48, "text": " those layers."}, {"title": "Introduction to Deep Learning", "start": 2466.48, "end": 2470.3199999999997, "text": " Because if you don't have that nonlinearity, it turns out that a linear layer and another"}, {"title": "Introduction to Deep Learning", "start": 2470.3199999999997, "end": 2473.2, "text": " linear layer, this is just a linear combination."}, {"title": "Introduction to Deep Learning", "start": 2473.2, "end": 2474.2, "text": " It's still linear."}, {"title": "Introduction to Deep Learning", "start": 2474.2, "end": 2476.4399999999996, "text": " So you need that nonlinearity."}, {"title": "Introduction to Deep Learning", "start": 2476.66, "end": 2478.78, "text": " But doing that, you can actually approximate anything."}, {"title": "Introduction to Deep Learning", "start": 2478.78, "end": 2486.38, "text": " And one way to think about it is given some amount of capacity, some number of different"}, {"title": "Introduction to Deep Learning", "start": 2486.38, "end": 2492.3, "text": " dimensions that you can do, you can actually approximate any function."}, {"title": "Introduction to Deep Learning", "start": 2492.3, "end": 2495.58, "text": " This is kind of a rough, hand-wavy intuition thing."}, {"title": "Introduction to Deep Learning", "start": 2495.58, "end": 2505.86, "text": " But as you're shrinking the size of these different vertical things, we're doing something"}, {"title": "Introduction to Deep Learning", "start": 2506.28, "end": 2507.28, "text": " like a Riemann sum."}, {"title": "Introduction to Deep Learning", "start": 2507.28, "end": 2510.6400000000003, "text": " You can approximate any integral as long as you have enough capacity in terms of the number"}, {"title": "Introduction to Deep Learning", "start": 2510.6400000000003, "end": 2513.04, "text": " of small things that you're building up."}, {"title": "Introduction to Deep Learning", "start": 2513.04, "end": 2517.6, "text": " So this gets at basically one of the issues, which is efficiency."}, {"title": "Introduction to Deep Learning", "start": 2517.6, "end": 2524.0, "text": " So in theory, you can approximate any complicated function with a sufficiently wide two-layer"}, {"title": "Introduction to Deep Learning", "start": 2524.0, "end": 2525.0, "text": " network."}, {"title": "Introduction to Deep Learning", "start": 2525.0, "end": 2529.76, "text": " So this means not two dimensions, maybe thousands or millions of dimensions."}, {"title": "Introduction to Deep Learning", "start": 2529.76, "end": 2533.82, "text": " But in practice, that's actually very inefficient."}, {"title": "Introduction to Deep Learning", "start": 2533.82, "end": 2539.36, "text": " So a narrow, deep model may be something that only has much fewer dimensions in terms"}, {"title": "Introduction to Deep Learning", "start": 2539.36, "end": 2542.46, "text": " of the size of each vector or each input and output."}, {"title": "Introduction to Deep Learning", "start": 2542.46, "end": 2548.36, "text": " But now, many, many, many more stacked layers, you might be able to, with a lot less parameters,"}, {"title": "Introduction to Deep Learning", "start": 2548.36, "end": 2550.6800000000003, "text": " approximate the same complex function."}, {"title": "Introduction to Deep Learning", "start": 2550.6800000000003, "end": 2552.1600000000003, "text": " In practice, we do find that's true."}, {"title": "Introduction to Deep Learning", "start": 2552.1600000000003, "end": 2554.7200000000003, "text": " More layers helps."}, {"title": "Introduction to Deep Learning", "start": 2554.7200000000003, "end": 2558.06, "text": " And we'll get into this a little bit in lecture three, which is about approximation theory,"}, {"title": "Introduction to Deep Learning", "start": 2558.06, "end": 2562.2000000000003, "text": " the theory of what it is possible to approximate."}, {"title": "Introduction to Deep Learning", "start": 2562.22, "end": 2565.58, "text": " And then the other thing that we're going to cover in this class is architectures."}, {"title": "Introduction to Deep Learning", "start": 2565.58, "end": 2567.8199999999997, "text": " So we talk about deep networks."}, {"title": "Introduction to Deep Learning", "start": 2567.8199999999997, "end": 2572.66, "text": " Really often, we're talking about something that's roughly like this, some cascade of"}, {"title": "Introduction to Deep Learning", "start": 2572.66, "end": 2578.98, "text": " repeated simple computations, each of these linear and then linear models with a non-linearity."}, {"title": "Introduction to Deep Learning", "start": 2578.98, "end": 2583.4399999999996, "text": " And then with every increasing layer, we're getting more abstracted representations."}, {"title": "Introduction to Deep Learning", "start": 2583.4399999999996, "end": 2585.98, "text": " You're getting a higher capacity model."}, {"title": "Introduction to Deep Learning", "start": 2585.98, "end": 2592.06, "text": " And this means it can form more complex and abstract calculations, whereas a shallow network,"}, {"title": "Introduction to Deep Learning", "start": 2592.92, "end": 2597.0, "text": " if it's not sufficiently wide, could maybe only efficiently compute simple things, like"}, {"title": "Introduction to Deep Learning", "start": 2597.0, "end": 2598.56, "text": " maybe finding edges."}, {"title": "Introduction to Deep Learning", "start": 2598.56, "end": 2603.7, "text": " A deep network could efficiently compute harder things, like recognizing this clownfish or"}, {"title": "Introduction to Deep Learning", "start": 2603.7, "end": 2607.2799999999997, "text": " translating English to Chinese."}, {"title": "Introduction to Deep Learning", "start": 2607.2799999999997, "end": 2612.32, "text": " And so this classification of complex things, essentially, your whole function f of x is"}, {"title": "Introduction to Deep Learning", "start": 2612.32, "end": 2619.24, "text": " now this sort of stacked or almost like chain-like mapping from your final layer, which is then"}, {"title": "Introduction to Deep Learning", "start": 2619.2599999999998, "end": 2624.5, "text": " a functional mapping of your previous layer, and in, and in, and in, and in, all the way"}, {"title": "Introduction to Deep Learning", "start": 2624.5, "end": 2626.3799999999997, "text": " back to that input."}, {"title": "Introduction to Deep Learning", "start": 2626.3799999999997, "end": 2630.22, "text": " And there's a lot of different ways that we've learned to build these mappings for different"}, {"title": "Introduction to Deep Learning", "start": 2630.22, "end": 2634.3999999999996, "text": " types of problems and based on the structure of the data we're trying to model."}, {"title": "Introduction to Deep Learning", "start": 2634.3999999999996, "end": 2635.3999999999996, "text": " And we'll get into that."}, {"title": "Introduction to Deep Learning", "start": 2635.3999999999996, "end": 2638.58, "text": " We'll talk about CNNs, GNNs, transformers, and RNNs."}, {"title": "Introduction to Deep Learning", "start": 2638.58, "end": 2643.66, "text": " So convolutional neural networks, graph neural networks, transformers, and recurrent neural"}, {"title": "Introduction to Deep Learning", "start": 2643.66, "end": 2645.74, "text": " networks."}, {"title": "Introduction to Deep Learning", "start": 2645.74, "end": 2648.64, "text": " And then the other thing that we're going to be talking about in this class is when"}, {"title": "Introduction to Deep Learning", "start": 2648.64, "end": 2652.3199999999997, "text": " and why we can generalize."}, {"title": "Introduction to Deep Learning", "start": 2652.3199999999997, "end": 2653.7999999999997, "text": " So why do deep nets generalize?"}, {"title": "Introduction to Deep Learning", "start": 2653.7999999999997, "end": 2658.3199999999997, "text": " Essentially, deep nets have so many parameters, they could just act like lookup tables."}, {"title": "Introduction to Deep Learning", "start": 2658.3199999999997, "end": 2660.4799999999996, "text": " You just regurgitate the training data."}, {"title": "Introduction to Deep Learning", "start": 2660.4799999999996, "end": 2663.2799999999997, "text": " But instead, they seem to learn rules that generalize."}, {"title": "Introduction to Deep Learning", "start": 2663.2799999999997, "end": 2665.16, "text": " And this actually defies classical theory."}, {"title": "Introduction to Deep Learning", "start": 2665.16, "end": 2671.9599999999996, "text": " So the classical theory basically says that if your model is over-parameterized, it will"}, {"title": "Introduction to Deep Learning", "start": 2671.9599999999996, "end": 2673.16, "text": " overfit."}, {"title": "Introduction to Deep Learning", "start": 2673.58, "end": 2680.1, "text": " So there's this capacity versus risk or error curve where if you don't have enough capacity,"}, {"title": "Introduction to Deep Learning", "start": 2680.1, "end": 2681.98, "text": " you're going to underfit the model."}, {"title": "Introduction to Deep Learning", "start": 2681.98, "end": 2685.1, "text": " And then with too much capacity, you're going to overfit."}, {"title": "Introduction to Deep Learning", "start": 2685.1, "end": 2691.42, "text": " But in practice, this is from this paper Double Descent, what we see is that in this modern"}, {"title": "Introduction to Deep Learning", "start": 2691.42, "end": 2695.62, "text": " interpolating regime, we go past that point of being over-parameterized."}, {"title": "Introduction to Deep Learning", "start": 2695.62, "end": 2701.94, "text": " And we're actually then able to actually pass some interpolation threshold or ability to"}, {"title": "Introduction to Deep Learning", "start": 2701.96, "end": 2705.36, "text": " have enough capacity in the model to interpolate between the data points."}, {"title": "Introduction to Deep Learning", "start": 2705.36, "end": 2709.68, "text": " We're actually able to then potentially get even better, even though the models are massively"}, {"title": "Introduction to Deep Learning", "start": 2709.68, "end": 2710.68, "text": " over-parameterized."}, {"title": "Introduction to Deep Learning", "start": 2710.68, "end": 2711.68, "text": " Yeah?"}, {"title": "Introduction to Deep Learning", "start": 2711.68, "end": 2712.68, "text": " What do you mean by capacity?"}, {"title": "Introduction to Deep Learning", "start": 2712.68, "end": 2715.44, "text": " Capacity is the number of parameters in the model."}, {"title": "Introduction to Deep Learning", "start": 2715.44, "end": 2717.76, "text": " So that's both the width and the depth."}, {"title": "Introduction to Deep Learning", "start": 2717.76, "end": 2721.64, "text": " Basically how many values are captured in those model weights."}, {"title": "Introduction to Deep Learning", "start": 2721.64, "end": 2722.64, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 2722.64, "end": 2723.64, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 2723.64, "end": 2724.64, "text": " Yes, thank you."}, {"title": "Introduction to Deep Learning", "start": 2724.64, "end": 2725.64, "text": " And how does that relate to the size of the data samples that you're collecting?"}, {"title": "Introduction to Deep Learning", "start": 2726.64, "end": 2727.64, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 2727.64, "end": 2740.2599999999998, "text": " So that is not a dimension on this plot, but it definitely is also related."}, {"title": "Introduction to Deep Learning", "start": 2740.2599999999998, "end": 2744.64, "text": " So if you have, but it's related in a different dimension."}, {"title": "Introduction to Deep Learning", "start": 2744.64, "end": 2750.66, "text": " If you're learning from fewer data points, it's very easy to learn to maybe overfit to"}, {"title": "Introduction to Deep Learning", "start": 2750.66, "end": 2751.66, "text": " those data points."}, {"title": "Introduction to Deep Learning", "start": 2751.72, "end": 2755.68, "text": " So you could kind of think of that, honestly, in another way maybe as being a different"}, {"title": "Introduction to Deep Learning", "start": 2755.68, "end": 2758.92, "text": " dimension of capacity here, where now it's like almost the capacity of your training"}, {"title": "Introduction to Deep Learning", "start": 2758.92, "end": 2762.7, "text": " data, though of course there's not like a direct translation."}, {"title": "Introduction to Deep Learning", "start": 2762.7, "end": 2766.06, "text": " But so if you don't have enough training data, you also can't learn to interpolate between"}, {"title": "Introduction to Deep Learning", "start": 2766.06, "end": 2772.3399999999997, "text": " those data points because you don't have enough coverage of the data distribution."}, {"title": "Introduction to Deep Learning", "start": 2772.3399999999997, "end": 2773.3799999999997, "text": " I live in that world a lot."}, {"title": "Introduction to Deep Learning", "start": 2773.3799999999997, "end": 2778.58, "text": " We often don't have enough data to kind of learn a general representation without a lot"}, {"title": "Introduction to Deep Learning", "start": 2778.58, "end": 2779.92, "text": " of handy tricks."}, {"title": "Introduction to Deep Learning", "start": 2779.92, "end": 2782.96, "text": " And we will talk about some of those different tricks in this class, some of the different"}, {"title": "Introduction to Deep Learning", "start": 2782.96, "end": 2788.0, "text": " ways that we try to convince models not to overfit to data and try to learn to sort of"}, {"title": "Introduction to Deep Learning", "start": 2788.0, "end": 2792.36, "text": " generalize beyond maybe what is kind of captured in the data."}, {"title": "Introduction to Deep Learning", "start": 2792.36, "end": 2794.28, "text": " But I think that dimension is really important."}, {"title": "Introduction to Deep Learning", "start": 2794.28, "end": 2797.9, "text": " It's one of the reasons that none of this was possible until we started building big"}, {"title": "Introduction to Deep Learning", "start": 2797.9, "end": 2802.56, "text": " enough data sets where you sort of had enough coverage of the space that you wanted to model"}, {"title": "Introduction to Deep Learning", "start": 2802.56, "end": 2806.16, "text": " in your data to be able to start interpolating between them."}, {"title": "Introduction to Deep Learning", "start": 2806.16, "end": 2807.16, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 2807.3999999999996, "end": 2811.3999999999996, "text": " Can you clarify the difference between overfitting and overparameterized?"}, {"title": "Introduction to Deep Learning", "start": 2811.3999999999996, "end": 2814.8799999999997, "text": " Oh, yeah."}, {"title": "Introduction to Deep Learning", "start": 2814.8799999999997, "end": 2818.3199999999997, "text": " So I mean, I think this paper would be great to read to be able to really get into it."}, {"title": "Introduction to Deep Learning", "start": 2818.3199999999997, "end": 2823.24, "text": " But essentially, the idea here is that in the previous literature, we sort of said that"}, {"title": "Introduction to Deep Learning", "start": 2823.24, "end": 2828.16, "text": " at some point, if you had kind of too many parameters, you were going to overfit and"}, {"title": "Introduction to Deep Learning", "start": 2828.16, "end": 2831.14, "text": " learn a model that actually generalized worse."}, {"title": "Introduction to Deep Learning", "start": 2831.14, "end": 2835.7999999999997, "text": " Because based on the amount of data that you had, it was going to kind of, there's like"}, {"title": "Introduction to Deep Learning", "start": 2835.8, "end": 2838.6400000000003, "text": " a classical example where you have like three data points."}, {"title": "Introduction to Deep Learning", "start": 2838.6400000000003, "end": 2844.84, "text": " And if you try to learn like a 12-dimensional function, it will learn something like this"}, {"title": "Introduction to Deep Learning", "start": 2844.84, "end": 2846.6000000000004, "text": " to fit those data points perfectly."}, {"title": "Introduction to Deep Learning", "start": 2846.6000000000004, "end": 2852.36, "text": " But actually, it's not likely to generalize because it's basically too spiky or too peaky."}, {"title": "Introduction to Deep Learning", "start": 2852.36, "end": 2854.48, "text": " So that's the idea of overfitting."}, {"title": "Introduction to Deep Learning", "start": 2854.48, "end": 2858.44, "text": " But this idea of like overparameterized is basically instead of saying underfitting and"}, {"title": "Introduction to Deep Learning", "start": 2858.44, "end": 2863.88, "text": " overfitting, now we're saying that the new paradigm is underparameterized and overparameterized."}, {"title": "Introduction to Deep Learning", "start": 2863.96, "end": 2870.48, "text": " And what they're basically saying is that you maybe can't be too overparameterized."}, {"title": "Introduction to Deep Learning", "start": 2870.48, "end": 2876.1600000000003, "text": " Though this perspective is one that doesn't take into account resources."}, {"title": "Introduction to Deep Learning", "start": 2876.1600000000003, "end": 2882.36, "text": " So there's many, many people in places and places that are at MIT, but even actually"}, {"title": "Introduction to Deep Learning", "start": 2882.36, "end": 2885.2400000000002, "text": " in this class where we don't have infinite compute resources."}, {"title": "Introduction to Deep Learning", "start": 2885.2400000000002, "end": 2889.04, "text": " So actually, what you really want is some optimal point on this curve where you get"}, {"title": "Introduction to Deep Learning", "start": 2889.04, "end": 2894.84, "text": " good performance, but you're not maybe using resources that you don't have, like super,"}, {"title": "Introduction to Deep Learning", "start": 2894.84, "end": 2895.84, "text": " super large memory GPUs."}, {"title": "Introduction to Deep Learning", "start": 2895.84, "end": 2896.84, "text": " Yeah?"}, {"title": "Introduction to Deep Learning", "start": 2896.84, "end": 2897.84, "text": " When you gave the example of using two very wide layers versus multiple narrow ones, has"}, {"title": "Introduction to Deep Learning", "start": 2897.84, "end": 2898.84, "text": " any of this evolved about which is better for about like interpretability between the"}, {"title": "Introduction to Deep Learning", "start": 2898.84, "end": 2899.84, "text": " two?"}, {"title": "Introduction to Deep Learning", "start": 2899.84, "end": 2900.84, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 2900.84, "end": 2901.84, "text": " OK."}, {"title": "Introduction to Deep Learning", "start": 2901.84, "end": 2917.96, "text": " We're definitely going to get into this a lot more in some of the later lectures, kind"}, {"title": "Introduction to Deep Learning", "start": 2917.96, "end": 2924.84, "text": " of like these choices between breadth versus depth in bottles."}, {"title": "Introduction to Deep Learning", "start": 2924.84, "end": 2930.6, "text": " But at a very high level, your intuition I think that you're getting at is kind of correct."}, {"title": "Introduction to Deep Learning", "start": 2930.6, "end": 2935.84, "text": " If you have only two layers, actually, maybe not."}, {"title": "Introduction to Deep Learning", "start": 2935.84, "end": 2940.7200000000003, "text": " Because if you have two almost infinite parameter layers, really, really high dimensional space"}, {"title": "Introduction to Deep Learning", "start": 2940.7200000000003, "end": 2942.68, "text": " is hard to interpret."}, {"title": "Introduction to Deep Learning", "start": 2942.68, "end": 2949.08, "text": " If you have lower dimensional space over many, many different now stacked versions of these"}, {"title": "Introduction to Deep Learning", "start": 2949.08, "end": 2951.16, "text": " layers, it's also hard to interpret."}, {"title": "Introduction to Deep Learning", "start": 2951.16, "end": 2956.3999999999996, "text": " So I would say like in ecology, for example, people often still use just generative additive"}, {"title": "Introduction to Deep Learning", "start": 2956.3999999999996, "end": 2960.2799999999997, "text": " models of a bunch of different linear things because they need the interpretability or"}, {"title": "Introduction to Deep Learning", "start": 2960.2799999999997, "end": 2963.08, "text": " they feel like they do."}, {"title": "Introduction to Deep Learning", "start": 2963.08, "end": 2968.9199999999996, "text": " And any times you get to large numbers of parameters in either dimension, that interpretability"}, {"title": "Introduction to Deep Learning", "start": 2968.9199999999996, "end": 2969.9199999999996, "text": " gets more difficult."}, {"title": "Introduction to Deep Learning", "start": 2969.92, "end": 2977.2000000000003, "text": " I was just going to add that even in like, let's say, open AI, it's like a closely guarded"}, {"title": "Introduction to Deep Learning", "start": 2977.2000000000003, "end": 2983.88, "text": " secret like what recipe of like width versus depth that they use in like GPT whatever."}, {"title": "Introduction to Deep Learning", "start": 2983.88, "end": 2987.84, "text": " If you can come up with a really scientific way to answer that question, you could go"}, {"title": "Introduction to Deep Learning", "start": 2987.84, "end": 2989.64, "text": " and get a job there and like."}, {"title": "Introduction to Deep Learning", "start": 2989.64, "end": 2990.64, "text": " Make seven figures?"}, {"title": "Introduction to Deep Learning", "start": 2990.64, "end": 2991.64, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 2991.64, "end": 2997.76, "text": " So what Jeremy is saying is like there's no perfect prescription or recipe for width versus"}, {"title": "Introduction to Deep Learning", "start": 2997.76, "end": 2998.76, "text": " depth and what's optimal."}, {"title": "Introduction to Deep Learning", "start": 2998.76, "end": 2999.76, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 2999.76, "end": 3007.44, "text": " So the simplicity hypothesis in classical theory basically says that big models learn"}, {"title": "Introduction to Deep Learning", "start": 3007.44, "end": 3010.1200000000003, "text": " complicated functions and overfit the data."}, {"title": "Introduction to Deep Learning", "start": 3010.1200000000003, "end": 3016.32, "text": " And the immersion theory is that deep networks actually learn simple functions that generalize."}, {"title": "Introduction to Deep Learning", "start": 3016.32, "end": 3020.0800000000004, "text": " And we're going to talk about this a lot more in a theoretical and a more experimental lecture"}, {"title": "Introduction to Deep Learning", "start": 3020.0800000000004, "end": 3023.96, "text": " around generalization, both in and out of distribution."}, {"title": "Introduction to Deep Learning", "start": 3023.96, "end": 3025.0200000000004, "text": " OK."}, {"title": "Introduction to Deep Learning", "start": 3025.0200000000004, "end": 3029.32, "text": " So again, what I've expected you to see before, softmax cross-entropy loss."}, {"title": "Introduction to Deep Learning", "start": 3029.88, "end": 3033.7200000000003, "text": " Basically, we have this model that we're trying to use to learn to classify something like"}, {"title": "Introduction to Deep Learning", "start": 3033.7200000000003, "end": 3034.7200000000003, "text": " clownfish."}, {"title": "Introduction to Deep Learning", "start": 3034.7200000000003, "end": 3038.76, "text": " And we assume that you've seen some of these really basic loss functions that we use for"}, {"title": "Introduction to Deep Learning", "start": 3038.76, "end": 3039.76, "text": " this."}, {"title": "Introduction to Deep Learning", "start": 3039.76, "end": 3045.0800000000004, "text": " So for example, if you have this as your last layer and now you have some set of categories"}, {"title": "Introduction to Deep Learning", "start": 3045.0800000000004, "end": 3049.0800000000004, "text": " you're trying to learn to categorize over, the simple version of this is you take the"}, {"title": "Introduction to Deep Learning", "start": 3049.0800000000004, "end": 3054.6800000000003, "text": " argmax and you say, OK, well, the thing that's darkest on here is clownfish."}, {"title": "Introduction to Deep Learning", "start": 3054.68, "end": 3059.7599999999998, "text": " And then the loss that you would calculate is, OK, the actual correct ground truth label"}, {"title": "Introduction to Deep Learning", "start": 3059.7599999999998, "end": 3060.7599999999998, "text": " is clownfish."}, {"title": "Introduction to Deep Learning", "start": 3060.7599999999998, "end": 3064.62, "text": " And then the error between those two, in this case, is 0."}, {"title": "Introduction to Deep Learning", "start": 3064.62, "end": 3065.62, "text": " So it's small."}, {"title": "Introduction to Deep Learning", "start": 3065.62, "end": 3069.12, "text": " So you've gotten the correct prediction."}, {"title": "Introduction to Deep Learning", "start": 3069.12, "end": 3073.52, "text": " And then maybe actually the real ground truth label is grizzly bear."}, {"title": "Introduction to Deep Learning", "start": 3073.52, "end": 3075.0, "text": " And you've said clownfish."}, {"title": "Introduction to Deep Learning", "start": 3075.0, "end": 3076.8399999999997, "text": " Now what you'd want is for your loss to be large."}, {"title": "Introduction to Deep Learning", "start": 3076.8399999999997, "end": 3081.3999999999996, "text": " So you want some loss function that does a good job of punishing you when you're wrong"}, {"title": "Introduction to Deep Learning", "start": 3081.64, "end": 3084.88, "text": " and not punishing you when you're right."}, {"title": "Introduction to Deep Learning", "start": 3084.88, "end": 3090.84, "text": " And the way we actually do this often is if you assume the net output is some normalized"}, {"title": "Introduction to Deep Learning", "start": 3090.84, "end": 3095.48, "text": " vector, then you can kind of think of this as a probability distribution over different"}, {"title": "Introduction to Deep Learning", "start": 3095.48, "end": 3097.52, "text": " possible object classes."}, {"title": "Introduction to Deep Learning", "start": 3097.52, "end": 3102.2400000000002, "text": " And so what you want, this loss function on the right, this is called cross-entropy loss."}, {"title": "Introduction to Deep Learning", "start": 3102.2400000000002, "end": 3107.36, "text": " This essentially indicates the distance between what the model believes the output distribution"}, {"title": "Introduction to Deep Learning", "start": 3107.36, "end": 3112.28, "text": " should be and what the input distribution actually is, which in this case is what we"}, {"title": "Introduction to Deep Learning", "start": 3112.28, "end": 3117.04, "text": " call a one-hot encoding or essentially a vector that's 0 everywhere except for the place where"}, {"title": "Introduction to Deep Learning", "start": 3117.04, "end": 3119.8, "text": " it's correct, where it's 1."}, {"title": "Introduction to Deep Learning", "start": 3119.8, "end": 3123.08, "text": " And so essentially, it's just telling the network to maximize the probability of the"}, {"title": "Introduction to Deep Learning", "start": 3123.08, "end": 3128.08, "text": " training data, which forces the output to be a reasonably good probability model of"}, {"title": "Introduction to Deep Learning", "start": 3128.08, "end": 3132.44, "text": " the object class given the image, assuming you have enough training data."}, {"title": "Introduction to Deep Learning", "start": 3132.44, "end": 3135.2000000000003, "text": " So what this looks like in practice, you say clownfish."}, {"title": "Introduction to Deep Learning", "start": 3135.2, "end": 3138.2799999999997, "text": " The ground truth label is clownfish."}, {"title": "Introduction to Deep Learning", "start": 3138.2799999999997, "end": 3140.08, "text": " Maybe this is your prediction."}, {"title": "Introduction to Deep Learning", "start": 3140.08, "end": 3147.2799999999997, "text": " So now the score is basically saying, this is how much better you could have done, how"}, {"title": "Introduction to Deep Learning", "start": 3147.2799999999997, "end": 3151.64, "text": " much better you could have done, which is the difference between the amount of probability"}, {"title": "Introduction to Deep Learning", "start": 3151.64, "end": 3153.08, "text": " you gave the class clownfish."}, {"title": "Introduction to Deep Learning", "start": 3153.08, "end": 3154.3999999999996, "text": " I'm putting probability in quotes here."}, {"title": "Introduction to Deep Learning", "start": 3154.3999999999996, "end": 3157.24, "text": " It's super important to know that these are not true probabilities."}, {"title": "Introduction to Deep Learning", "start": 3157.24, "end": 3159.64, "text": " They're scores."}, {"title": "Introduction to Deep Learning", "start": 3159.64, "end": 3164.72, "text": " And then that red is kind of the dimension that you could do better, and that's telling"}, {"title": "Introduction to Deep Learning", "start": 3164.72, "end": 3168.12, "text": " you the direction of the gradient you want to go in."}, {"title": "Introduction to Deep Learning", "start": 3168.12, "end": 3170.9199999999996, "text": " So maybe grizzly bear, you got it correct."}, {"title": "Introduction to Deep Learning", "start": 3170.9199999999996, "end": 3171.9199999999996, "text": " It's grizzly bear."}, {"title": "Introduction to Deep Learning", "start": 3171.9199999999996, "end": 3181.9199999999996, "text": " Now the amount that's left over is small versus actually if it's chameleon and you're saying"}, {"title": "Introduction to Deep Learning", "start": 3181.9199999999996, "end": 3183.7999999999997, "text": " iguana, essentially this thing is normalized."}, {"title": "Introduction to Deep Learning", "start": 3183.7999999999997, "end": 3187.48, "text": " So if you put a lot more probability on one thing than the correct answer, then you end"}, {"title": "Introduction to Deep Learning", "start": 3187.48, "end": 3193.04, "text": " up with a much smaller amount than how you're looking at the difference between it."}, {"title": "Introduction to Deep Learning", "start": 3193.48, "end": 3198.8, "text": " The idea with all of this in deep learning is you're taking all these different weights"}, {"title": "Introduction to Deep Learning", "start": 3198.8, "end": 3204.6, "text": " in your massive model, and then you're fiddling around with them until you match the desired"}, {"title": "Introduction to Deep Learning", "start": 3204.6, "end": 3207.2599999999998, "text": " output for all your training data."}, {"title": "Introduction to Deep Learning", "start": 3207.2599999999998, "end": 3210.92, "text": " You're basically just wiggling around all these different weights."}, {"title": "Introduction to Deep Learning", "start": 3210.92, "end": 3215.36, "text": " So you're going through your training data, you're predicting something, you're calculating"}, {"title": "Introduction to Deep Learning", "start": 3215.36, "end": 3219.4, "text": " a loss, and you're doing this for many different data samples."}, {"title": "Introduction to Deep Learning", "start": 3219.4, "end": 3222.68, "text": " And you're doing it over and over and over again until the model gets everything right"}, {"title": "Introduction to Deep Learning", "start": 3223.3199999999997, "end": 3227.08, "text": " or as close to everything right as possible."}, {"title": "Introduction to Deep Learning", "start": 3227.08, "end": 3230.72, "text": " We also expect that you've seen some amount of parallel processing and the idea of tensors"}, {"title": "Introduction to Deep Learning", "start": 3230.72, "end": 3232.52, "text": " before this."}, {"title": "Introduction to Deep Learning", "start": 3232.52, "end": 3237.16, "text": " So this is the idea that a lot of this stuff, because it's repeated calculations, you can"}, {"title": "Introduction to Deep Learning", "start": 3237.16, "end": 3238.3999999999996, "text": " be done in batch."}, {"title": "Introduction to Deep Learning", "start": 3238.3999999999996, "end": 3242.48, "text": " So each of these individual losses for the individual data points, well, it's all going"}, {"title": "Introduction to Deep Learning", "start": 3242.48, "end": 3243.48, "text": " to be summed anyway."}, {"title": "Introduction to Deep Learning", "start": 3243.48, "end": 3248.08, "text": " So you can do it in parallel, and now you can take these batches and more efficiently"}, {"title": "Introduction to Deep Learning", "start": 3248.08, "end": 3250.22, "text": " calculate that sum."}, {"title": "Introduction to Deep Learning", "start": 3250.2599999999998, "end": 3255.66, "text": " And so look, you take the same layer for all three of these, you can stack them, and now"}, {"title": "Introduction to Deep Learning", "start": 3255.66, "end": 3260.66, "text": " essentially you get this matrix that's featured, it's multiplied by images."}, {"title": "Introduction to Deep Learning", "start": 3260.66, "end": 3262.4199999999996, "text": " And so that's what your tensors are."}, {"title": "Introduction to Deep Learning", "start": 3262.4199999999996, "end": 3266.98, "text": " They're these multidimensional arrays, and every single layer is some representation"}, {"title": "Introduction to Deep Learning", "start": 3266.98, "end": 3269.48, "text": " of the input data."}, {"title": "Introduction to Deep Learning", "start": 3269.48, "end": 3271.1, "text": " And actually, everything is a tensor."}, {"title": "Introduction to Deep Learning", "start": 3271.1, "end": 3273.64, "text": " A tensor is just a multidimensional matrix."}, {"title": "Introduction to Deep Learning", "start": 3273.64, "end": 3278.4399999999996, "text": " So you can say, OK, here's our mathematical representation of this model."}, {"title": "Introduction to Deep Learning", "start": 3278.44, "end": 3280.92, "text": " You can also just represent it this way."}, {"title": "Introduction to Deep Learning", "start": 3280.92, "end": 3286.4, "text": " You have these vectors of your different input values, and now you have your batch dimension,"}, {"title": "Introduction to Deep Learning", "start": 3286.4, "end": 3289.84, "text": " which is now this vertical stacking of all those layers."}, {"title": "Introduction to Deep Learning", "start": 3289.84, "end": 3293.7200000000003, "text": " And now you multiply that by your weights matrix, and you get out your output."}, {"title": "Introduction to Deep Learning", "start": 3293.7200000000003, "end": 3295.88, "text": " And then you run that through a pointwise non-linearity."}, {"title": "Introduction to Deep Learning", "start": 3295.88, "end": 3300.4, "text": " And then again, just another matrix multiplication, and then you get your output z2, and then"}, {"title": "Introduction to Deep Learning", "start": 3300.4, "end": 3302.48, "text": " you take that non-linearity, you get y."}, {"title": "Introduction to Deep Learning", "start": 3302.48, "end": 3308.32, "text": " So this is why the fact that GPUs can do so many multiplications in parallel was important."}, {"title": "Introduction to Deep Learning", "start": 3309.2000000000003, "end": 3312.88, "text": " Because we can rewrite all of these models as just essentially sets of multiplications."}, {"title": "Introduction to Deep Learning", "start": 3312.88, "end": 3316.48, "text": " And we'll go into this in a lot more detail as well in the future."}, {"title": "Introduction to Deep Learning", "start": 3316.48, "end": 3320.8, "text": " So what we're going to cover in this class is how deep networks represent data."}, {"title": "Introduction to Deep Learning", "start": 3320.8, "end": 3332.56, "text": " So essentially, it's kind of this idea, where deep networks are a more compact way of representing"}, {"title": "Introduction to Deep Learning", "start": 3332.56, "end": 3334.84, "text": " knowledge in data."}, {"title": "Introduction to Deep Learning", "start": 3334.84, "end": 3339.6400000000003, "text": " And previous methods we saw were kind of more like lookup tables."}, {"title": "Introduction to Deep Learning", "start": 3339.6400000000003, "end": 3343.88, "text": " But there's other and maybe better ways to learn a mapping between input and output."}, {"title": "Introduction to Deep Learning", "start": 3343.88, "end": 3349.0, "text": " Basically assume that there are kind of these lower level building blocks that are useful"}, {"title": "Introduction to Deep Learning", "start": 3349.0, "end": 3351.84, "text": " for many different possible downstream tasks."}, {"title": "Introduction to Deep Learning", "start": 3351.84, "end": 3356.6800000000003, "text": " So one nice example is the idea of trying to learn a classifier for the letter T. So"}, {"title": "Introduction to Deep Learning", "start": 3356.6800000000003, "end": 3359.92, "text": " if you want to learn to classify the letter T, you could try to build a classifier just"}, {"title": "Introduction to Deep Learning", "start": 3359.92, "end": 3363.6000000000004, "text": " for the letter T. Or you could take a classifier that classifies"}, {"title": "Introduction to Deep Learning", "start": 3363.64, "end": 3364.64, "text": " lines."}, {"title": "Introduction to Deep Learning", "start": 3364.64, "end": 3366.64, "text": " And you could say, OK, here's a line."}, {"title": "Introduction to Deep Learning", "start": 3366.64, "end": 3368.96, "text": " And then you could have a rotation of that classifier."}, {"title": "Introduction to Deep Learning", "start": 3368.96, "end": 3370.68, "text": " And you'd say, here's a line."}, {"title": "Introduction to Deep Learning", "start": 3370.68, "end": 3375.12, "text": " And now all you have to do is understand the connection point between these two lines."}, {"title": "Introduction to Deep Learning", "start": 3375.12, "end": 3381.48, "text": " So that line classifier is reusable in some combination to get a T classifier."}, {"title": "Introduction to Deep Learning", "start": 3381.48, "end": 3382.8399999999997, "text": " And that's kind of the basic idea here."}, {"title": "Introduction to Deep Learning", "start": 3382.8399999999997, "end": 3390.24, "text": " And actually, in this post-hoc way, we've analyzed what is learned in the layers of"}, {"title": "Introduction to Deep Learning", "start": 3390.24, "end": 3391.7599999999998, "text": " things like convolutional neural networks."}, {"title": "Introduction to Deep Learning", "start": 3391.8, "end": 3395.88, "text": " And we do see that you'll get low level representations in the earlier layers."}, {"title": "Introduction to Deep Learning", "start": 3395.88, "end": 3402.48, "text": " Then they get combined into more and more complex representations in those later layers."}, {"title": "Introduction to Deep Learning", "start": 3402.48, "end": 3404.4, "text": " And you can actually kind of see this as well."}, {"title": "Introduction to Deep Learning", "start": 3404.4, "end": 3411.88, "text": " If you take an input image and you take those and you sort of cluster them in some low level"}, {"title": "Introduction to Deep Learning", "start": 3411.88, "end": 3416.48, "text": " representation of the space for each layer, the layers earlier on in the network are not"}, {"title": "Introduction to Deep Learning", "start": 3416.48, "end": 3419.96, "text": " going to cluster well by category because they're really just categorizing things like"}, {"title": "Introduction to Deep Learning", "start": 3420.0, "end": 3425.92, "text": " does it have directional orientations of light and dark gradients in the pixels."}, {"title": "Introduction to Deep Learning", "start": 3425.92, "end": 3429.8, "text": " But actually, where you get towards the end of the network, it's learning more concepts."}, {"title": "Introduction to Deep Learning", "start": 3429.8, "end": 3434.8, "text": " And that's where you start getting these clusters of different things like fish."}, {"title": "Introduction to Deep Learning", "start": 3434.8, "end": 3440.28, "text": " And we will sort of talk about actually how that representation learns and is sort of"}, {"title": "Introduction to Deep Learning", "start": 3440.28, "end": 3445.08, "text": " structured through space in a bunch of different lectures on representation learning where"}, {"title": "Introduction to Deep Learning", "start": 3445.08, "end": 3449.92, "text": " we talk about reconstruction, similarity, and theory."}, {"title": "Introduction to Deep Learning", "start": 3449.92, "end": 3453.52, "text": " We're also going to cover generative models going forward in the class."}, {"title": "Introduction to Deep Learning", "start": 3453.52, "end": 3456.6, "text": " And this is things like text-based and image-based generation."}, {"title": "Introduction to Deep Learning", "start": 3456.6, "end": 3462.96, "text": " And we'll cover basics, representations, and conditional models."}, {"title": "Introduction to Deep Learning", "start": 3462.96, "end": 3467.04, "text": " And then we're going to cover in this class how to think about weight reuse and some of"}, {"title": "Introduction to Deep Learning", "start": 3467.04, "end": 3471.8, "text": " the efficiencies in terms of training if you're able to reuse components of previously trained"}, {"title": "Introduction to Deep Learning", "start": 3471.8, "end": 3473.96, "text": " models."}, {"title": "Introduction to Deep Learning", "start": 3474.0, "end": 3480.56, "text": " So here, again, this idea that like, all right, well, you learned this thing to categorize"}, {"title": "Introduction to Deep Learning", "start": 3480.56, "end": 3482.52, "text": " pictures of animals."}, {"title": "Introduction to Deep Learning", "start": 3482.52, "end": 3486.16, "text": " And now maybe you want to categorize some sort of satellite imagery."}, {"title": "Introduction to Deep Learning", "start": 3486.16, "end": 3493.0, "text": " Well, actually, a lot of those initial components about lines and orientations and structures,"}, {"title": "Introduction to Deep Learning", "start": 3493.0, "end": 3496.2, "text": " those are useful for both of these."}, {"title": "Introduction to Deep Learning", "start": 3496.2, "end": 3500.88, "text": " So what do we really need to learn from scratch versus what is kind of generally useful in"}, {"title": "Introduction to Deep Learning", "start": 3500.88, "end": 3504.6, "text": " terms of the representations that are learned?"}, {"title": "Introduction to Deep Learning", "start": 3504.6, "end": 3509.44, "text": " And this is really valuable if you don't have big data or big compute, if you can kind of"}, {"title": "Introduction to Deep Learning", "start": 3509.44, "end": 3514.08, "text": " pre-generate representations that then you can learn on top of without needing to learn"}, {"title": "Introduction to Deep Learning", "start": 3514.08, "end": 3517.1600000000003, "text": " everything from scratch."}, {"title": "Introduction to Deep Learning", "start": 3517.1600000000003, "end": 3522.7200000000003, "text": " And so we'll get into whether or not these things are generalizable or transferable."}, {"title": "Introduction to Deep Learning", "start": 3522.7200000000003, "end": 3526.6, "text": " In these transfer learning lectures, we talk about models and data."}, {"title": "Introduction to Deep Learning", "start": 3526.6, "end": 3529.96, "text": " And then finally, we'll talk about scaling."}, {"title": "Introduction to Deep Learning", "start": 3530.04, "end": 3536.96, "text": " So when you think about the scale of a brain, something like this little worm only has 302"}, {"title": "Introduction to Deep Learning", "start": 3536.96, "end": 3537.96, "text": " neurons."}, {"title": "Introduction to Deep Learning", "start": 3537.96, "end": 3544.68, "text": " So you can think of that as 302 different sort of values in its neural network."}, {"title": "Introduction to Deep Learning", "start": 3544.68, "end": 3546.92, "text": " Fruit fly is 15,000."}, {"title": "Introduction to Deep Learning", "start": 3546.92, "end": 3550.4, "text": " Human beings have 100 billion, roughly."}, {"title": "Introduction to Deep Learning", "start": 3550.4, "end": 3553.12, "text": " Elephants have 250 billion neurons in their brains."}, {"title": "Introduction to Deep Learning", "start": 3553.12, "end": 3554.8, "text": " I work with elephants a lot."}, {"title": "Introduction to Deep Learning", "start": 3554.8, "end": 3558.92, "text": " They're amazing."}, {"title": "Introduction to Deep Learning", "start": 3558.92, "end": 3563.2000000000003, "text": " And we'll talk about scale and deep learning and kind of what this means and actually get"}, {"title": "Introduction to Deep Learning", "start": 3563.2000000000003, "end": 3568.2000000000003, "text": " into some of the really great questions that different people asked in terms of how does"}, {"title": "Introduction to Deep Learning", "start": 3568.2000000000003, "end": 3571.28, "text": " maybe scaling rules change when you're thinking about optimization?"}, {"title": "Introduction to Deep Learning", "start": 3571.28, "end": 3573.2000000000003, "text": " What are the laws of scaling?"}, {"title": "Introduction to Deep Learning", "start": 3573.2000000000003, "end": 3578.6, "text": " How do we think about something like automatically learning how to best optimize a model?"}, {"title": "Introduction to Deep Learning", "start": 3578.6, "end": 3579.84, "text": " Great."}, {"title": "Introduction to Deep Learning", "start": 3579.84, "end": 3584.52, "text": " So tried to cover today how we got where we are today."}, {"title": "Introduction to Deep Learning", "start": 3584.52, "end": 3588.7200000000003, "text": " Basically just honestly mostly joking, but definitely like a brief history of sort of"}, {"title": "Introduction to Deep Learning", "start": 3589.52, "end": 3591.0, "text": " these fluctuation and hype cycles."}, {"title": "Introduction to Deep Learning", "start": 3591.0, "end": 3595.9199999999996, "text": " I would say that everyone's pretty clear that we're currently in like a really hypey part"}, {"title": "Introduction to Deep Learning", "start": 3595.9199999999996, "end": 3599.64, "text": " of the hype cycle."}, {"title": "Introduction to Deep Learning", "start": 3599.64, "end": 3604.08, "text": " What we expected you maybe saw before and also a very high level, some of the things"}, {"title": "Introduction to Deep Learning", "start": 3604.08, "end": 3606.8399999999997, "text": " we're going to cover in this class."}, {"title": "Introduction to Deep Learning", "start": 3606.8399999999997, "end": 3610.3199999999997, "text": " So I'm happy to, if anyone has any final questions."}, {"title": "Introduction to Deep Learning", "start": 3610.3199999999997, "end": 3611.3199999999997, "text": " Yeah."}, {"title": "Introduction to Deep Learning", "start": 3611.32, "end": 3626.6000000000004, "text": " How long it'll take the first lecture to get how many views?"}, {"title": "Introduction to Deep Learning", "start": 3626.6000000000004, "end": 3630.1200000000003, "text": " Well I mean honestly the intro lecture is probably going to be the least watched of"}, {"title": "Introduction to Deep Learning", "start": 3630.1200000000003, "end": 3634.56, "text": " all the lectures because it's just a bunch of course logistics."}, {"title": "Introduction to Deep Learning", "start": 3634.56, "end": 3636.52, "text": " We might even edit those out."}, {"title": "Introduction to Deep Learning", "start": 3636.52, "end": 3637.52, "text": " But great question."}, {"title": "Introduction to Deep Learning", "start": 3637.52, "end": 3638.32, "text": " I have no idea."}], "text": " So why are we all here? Deep learning has clearly been exploding in society. Machine learning generally is something that when I started studying it about 13 years ago didn't work. And now it works. So how many of you here in this room used AI in the last week? Yeah. Almost everybody. Probably everybody. And we're seeing massive advances in everything from AI-assisted text generation to 3D reconstruction, things like Nerf, things like AlphaGo, generating images, helping with writing code, playing games. This has really started to touch almost every dimension of society, and it's definitely very exciting to see. And I think that hopefully all of us will learn a lot more about what that kind of means in this course. So what is deep learning? First, I would argue that one necessary component of deep learning is the idea of neural networks. And these are a class of machine learning architectures that basically use stacks of linear transformations interleaved with pointwise nonlinearities. And these have really been a building block for a lot of the progress that we saw on the last slide. But the other really important dimension of deep learning is the idea of differential programming. And this is essentially a programming paradigm where we parameterize parts of the program and then let gradient-based optimization tune the parameters to basically optimize that program or find some sort of at least local optimal for that program. And these two things together is what we posit make up deep learning, and we're really going to dig into both of them in this course. So our philosophy for this is that breakthroughs in deep learning have been driven by a mixture of theory and practice, and both dimensions are really vital for future progress in the field. And so along those lines, this course should hopefully provide both a theoretical grounding in important deep learning building blocks as well as give you practice implementing, understanding, and using those blocks. OK, there's still a lot of people in the back who are packed in. I don't know if there's any way to come further forward. If not, we'll just do our best. So the class's coursework is going to be 65% problem sets. You'll have five p-sets. Each will be about one to two weeks long. And then there will be some combination of pen and paper, or more realistically, maybe overleaf, as well as code that you'll need to do and submit. And then 35% of the grade will be a final project. And this is a research project that's focused on deeper understanding of some of the topics that we cover in the course. You'll be asked to propose something for that project. So it'll be an initial project proposal. And then the final project will be a blog post that's going to demonstrate novel experimentation and visualization. And the hope here is that, I don't know, many of you, how many of you are machine learning researchers who have already written a machine learning paper, is first author or anywhere on the author list? OK, so some. These days, when you write a machine learning paper, more often than not, you also have to write a blog post about that paper. And that's intended to be catchy, make people understand the material, but really get them engaged. And so one of the reasons that we have this as the format of our final project is to reflect on the fact that this is the reality we live in today, that in machine learning research, being able to write a blog-style description of the work that you're doing and not reduce any of the technical complexity or the information that's given across, but just do this in this slightly more visual and maybe even interactive format, is a really valuable skill and one that we hope this can then help us think about. And we'll allow groups, but only up to two. And another important component, we are not able to provide compute. We may be able to provide some very limited compute, but that's TBD. So do not plan your final project to be something where you need to get state of the art by crunching a huge architecture on a bunch of huge data sets, because it's just not going to work. You're not going to be able to out-compete open AI on this research project. So this does not mean you can't do good research. And it's important to get creative here. I think it's actually a really valuable skill, particularly as certain types of large-scale deep learning become more and more centralized, to think about how you can still do impactful machine learning research in a way that is not just bigger is better. All right. Any questions about coursework or logistics? Yeah? Is the final project expected to be a mix of practical and theoretical research? So the question was, is the final project expected to be a mix of practical and theoretical research? I think it depends. It depends on what you want to propose as your final project. But I think we're pretty open. What we want is for them to be innovative. But that innovation can be in an applied sense, or it can be in a more theoretical sense. OK. So I'm going to do a really high-level overview of the schedule, just to give you a rough sense. So today, we're really covering basic intro stuff, course overview, a rough introduction to what neural networks are and basic building blocks, and particularly in the context of signposting what we expect you've seen before, because this is not an intro to deep learning class. This is advanced graduate-level deep learning. And then we're going to go into how you actually train a neural net, then approximation theory, some different architectures, things like grids, graphs. Then we'll hear from Jeremy about scaling rules for optimization. And then we'll look at generalization theory, more architectures, getting into transformers. We'll do a really fun lecture by Phil called The Hacker's Guide to Deep Learning that I think is quite useful. And then we'll talk about memory. And then we'll get into representation learning. So there, we're going to look at reconstruction-based, and then similarity-based, and then theory of representation learning. Then we'll have a little bit of a series on generative models, the basics, how representation learning interacts with generative modeling, thinking about conditional models. Then we'll talk about generalization, and particularly out-of-distribution generalization. We'll touch on transfer learning, both from a models and a data perspective. We'll hear about large language models. We'll have some guest lectures towards the end of the semester, and those are TBD. Then we'll hear about scaling laws, automatic gradient descent, and potentially the past and future of deep learning, though I think there's some discussion as to whether that's going to be a different lecture. And then I think, importantly, right towards the end, we will have class-wide project office hours where we'll have a bunch of the TAs and the instructors all come, and we can do open discussion around projects before they're due. So the next thing is that we're going to host PyTorch tutorials. This is intended for if you're not familiar with PyTorch and you think you need a refresher. And we strongly recommend that if this is the case, or if you even think it might be the case, that you attend one of the two PyTorch tutorials we're going to offer next week. So the question is if PyTorch is a requirement, are you allowed to use any other frameworks? So you're probably welcome to use any other frameworks in your final project, but some of the problem sets specifically will have code that's already built into PyTorch, and you're filling parts of that in. So unless you want to rewrite all of it in JAX or something, which I mean, more power to you, I would recommend that you definitely make sure that you are familiar with PyTorch. OK. Cool. So I'm going to talk about course policies, and this stuff is important, and it's also all on the web page. But I think, essentially, what we want to touch on is what our collaboration policy is first. So everyone is required to write an individual P-set. So when you're actually working on your problem sets, do not work on something together and then both submit the exact same thing. You can work on the actual content. You can discuss that with peers, TAs, and instructors, but the problem sets you submit need to be your own independent and individual work. And that also applies to the code that you write. So you can discuss the code with others, but you need to each write your own code separately. There was some confusion about this in the past. Make sure that you're not taking the code someone else wrote, running it, and then submitting a P-set on it. You should absolutely not copy or share complete solutions, and you also should not ask someone else, hey, this is my solution. Is it right? And you shouldn't do that in person, and you also shouldn't ask, is this correct on Piazza or Canvas? So, again, the idea is it's your independent work, your independent thought. And I think it matters more that you've gone through that process independently than actually getting it correct, because you'll learn from the feedback on your P-sets better if you submit something that you sort of came up with yourself and get feedback on it than if you copy somebody. And then, if you work with anyone on the P-set other than TAs and instructors, we ask that you write their names at the top of the P-set. So if you have a study group, you're all working together. We don't care if this is 10 people. We don't care if it's in this class 100 people, but we just want to know who was working together. I'd be impressed if you managed to come up with 100 person collaborative group. So AI assistants, we take the exact same policy about things like chat JPT or other AI assistants that we do with humans. So it's a deep learning class. You should be trying out the latest technology, right? The idea is not that we're teaching you to ignore what's happening, but we do want you to treat them like a human collaborator. Don't ask them to answer the question for you, right? Just try to imagine from an ethics perspective that this is like a peer in the class. So don't say, what is the solution to this? Don't say, write the code for me, because you wouldn't go to your friend and say, please write my code for me. But you're super welcome to use them as a discussion partner. Ask them questions that are contextual. Kind of go back and forth. That's all fine. Don't treat AI collaboration as if they're not sort of a human collaborator. Sort of think of it that way first. Like, is this something I would ask my friend in the class? And then just like you can come to office hours and ask a human questions about the lecture material, clarifications about questions, tips for getting started, you're welcome to do the same with AI assistants. Again, you're not allowed to ask an expert friend to do your homework for you. Don't ask AI to do your homework for you. If you're ever unclear, imagine AI as a human. Apply that same norm. If you work with any AI on a p-set, similarly to the fact that we're writing which humans we collaborated with, please write down which AI and how you used it. And of course, this is a bit vague, but again, just kind of on our code. Any questions about AI collaboration? Is that reasonably clear? Yeah? Yeah? I had a question on the last page. So can we show our work to the TAs? Yes, you're very welcome to show your work to the TAs. The TAs are not going to tell you how to answer the question though, but they will kind of work with you to help you hopefully figure it out on your own. Why are we here? What's the goal? So what do we want to do? Why are we all interested in things like deep learning? One perspective is that we're interested in modeling complex phenomena in the real world. What are complex phenomena? Well, you interact with them every day. Things like natural language, images, or visual data in general, videos. DNA, ecosystems are complex phenomena. Climate change is a complex phenomena. Why is it hard to model complex phenomena in the real world? They're complex. And so maybe one existence proof for deep learning as a solution to modeling complex phenomena, possibly we could talk about the human brain. All of you are here at MIT, which means you're pretty good at modeling complex phenomena in your brain, I'm guessing. So first today, I'm going to just briefly go over how we got where we are today. This is a brief history of the field of AI, deep learning in general. And then I'm going to go over what we expect you have already seen before. It's OK if you haven't seen these things before, but we would expect you then to go and brush up on them. So if there's anything that I talk about and you're like, oh, I've never seen that, we expect that. That's something you can go. Look at some of the open coursework courses at MIT. Go find lots of resources online and pick it up before you go into this class. And then I'll also talk about what we actually cover in the class. So a brief history of neural networks, and I think a fun way to go through this is looking at this on a plot of enthusiasm for neural networks over time. So if we go way back to 1958, Rosenblatt introduced the perceptron in psychological review. And the perceptron was the first neural network, arguably. And what this was intending to do was recognize or categorize images. And the idea was essentially that you could take some combination of representations, in this case, particularly pixels, sum them, put them through some non-linearity, and get out a categorization. And really, we'll see throughout the course and even today, this idea still forms the building block for most of the deep learning that we do. These perceptrons are components in many, many, many different types of capacities within things like Chat GPT. When this paper came out back in 1958, people were incredibly enthusiastic. It made waves. And then in 1972, Minsky and Papert wrote a book called Perceptrons, Expanded Edition. And this book takes a very critical lens to the idea that a perceptron could be seriously considered a model of the brain. And it also carefully and mathematically describes and characterizes the limitations of what a perceptron can model. And it did so, so carefully and so critically that all of a sudden, enthusiasm for machine learning really took a dip. So in the 70s, we were not excited about neural networks. But then in 1986, this book came out called Parallel Distributed Processing. And this was fundamental because it introduces the concept of backpropagation. And this was really the thing that enabled multi-layer perceptrons, so not just a single perceptron, but actually multiple perceptrons stacked on top of each other to actually be reasonably trained, reasonably fit, this concept of being able to backpropagate a gradient through multiple layers of perceptrons. And what this enabled you to solve were things like the XOR problem. For the first time, you could reasonably solve categorization problems that were not separable from a linear classifier. They pointed to backpropagation as a breakthrough. It let us actually handle things where you need multiple different boundaries and these non-linear boundaries to be drawn, which is only possible if you have multiple layers of a perceptron. You'll never be able to solve the XOR problem with a single layer neural network. And everyone got really excited again. So in the 80s, yet again, machine learning was something that people were pumped about, and particularly this deep learning neural network perspective on machine learning. In 1998, Jan LeCun put out Convolutional Neural Networks. There's an awesome demo that he has on his web page. I'm sure many of you have heard of Jan LeCun. And this seems like it would mean that everyone was really excited about neural networks again. But if you go to NURiPS in 2000, this is the premier conference on machine learning. It evolved from an interdisciplinary conference, which was on basically the science of the brain and machine learning, to specifically just a machine learning conference. If you look at that conference in 2000, the most predictive words and papers on the title for acceptance were belief propagation and Gaussian. And the title words most predictive of paper rejection were neural and network. So this is what we called the AI winter. And one of the reasons it was called the AI winter is because even though we had the theory, we had the building blocks, we didn't actually have the ability to train them efficiently. We didn't have the right programming perspective. And we didn't have the right hardware. But then in 2012, Alex Kurchevsky and his co-authors published AlexNet. And AlexNet was also another convolutional neural network. But importantly, Alex Kurchevsky was a brilliant programmer. And Alex Kurchevsky figured out how to program GPUs, graphics processing units, that have been developed to handle massive scale parallel multiplications for graphics, for video games, and for high resolution images. But he figured out how to program those and repurpose that hardware for the training of neural networks. And I can't tell you how much of a shot heard on the world this was in our field. But this, for the first time, outperformed every single other possible method that was out there on ImageNet. How many of you have heard of ImageNet? ImageNet was the first very large scale data set of its kind. And I actually think that this is an important component. What made machine learning start to work? And I would argue that, of course, it was the theory and the programming, the ability to actually sort of efficiently fit these things. But the other really vital component was the data. Machine learning does not work without large scale curated labeled data in many capacities still. So we're getting better and better at self-supervised learning. But so all of a sudden, we were back. Everyone was enthusiastic again. And actually, if you look, these were kind of hype cycles about 28 years, both of them. And so the question is, where are we going to be in 2028? So it's 2024 now. It's 2024. So in four years, where are we going to be? So I mean, one argument would be, OK, we're in another hype cycle. But I would actually argue that that's probably not true. Another one would be that we're just way off the deep end in terms of enthusiasm. And by 2028, we'll be out in the stratosphere. Or possibly, we're doing something like this. Maybe in 2028, we're going to hit some new sort of plane of enthusiasm. And then we'll start to realize limitations. And we'll oscillate again. Yeah? So the question is, will AI take over society by 2028? My answer is, let's see. So what's deep learning today? What are some of the components that make it up? First, I think a really important one is AutoGrad. These are coding languages like PyTorch and TensorFlow. They give us a really clever way to do things like implement the chain rule in software, making good use of available hardware. And this has enabled us to do things like approximate the gradient of any function, even if it's an approximation. So these are a really important component of deep learning. And if you do anything with deep learning, I doubt that you're doing what Kurchevsky did. I doubt that you're programming these things from scratch. You're almost certainly working with existing programming languages that are built for machine learning. It's billion-plus data point data sets, things like Lion. So that scale of data that I talked about before, this is an incredibly important component of all of this. Parallel training on thousands of GPUs. Again, this is what we're looking at today. Billion-plus parameter architectures, and this number just keeps increasing. Million-plus dollar training costs. So of course, it's not public knowledge how much it costs to train something like GPT-4. But I would bet it's definitely in the many millions. Shockingly good results. I don't know about any of you. I mean, I've been working in this field now for a while. I think we've been seeing results recently that I didn't anticipate seeing in my lifetime from when I started, back when things didn't work. So that's been both exciting to see and also sometimes, I think, for many of us, a bit overwhelming, right? Because you're like, oh gosh, the problems I thought were problems aren't problems anymore. So what are the problems, right? Of course, I work in the world of... I work a lot on biodiversity loss at a global scale using machine learning. And I would say that biodiversity loss is definitely still a problem, and one that machine learning is sometimes helping, but often also hurting, because the carbon costs of actually training these models is significant. But all of these massive things, massive isn't necessary. So things like stable diffusion are actually pretty lightweight and were incredibly impactful. So things don't always need to be big to be good. And then I think another really important component of deep learning today is the idea that it is primarily an open source community where we have what we call modular reuse, right? So people take weights that were trained by one person with one architecture, they might take that and use that entire thing as a module within another larger system that has many different components that are being pulled from different things. Now I think that this open source perspective is really something that's driven a lot of progress in machine learning, but also one of the things that we have been seeing in the last few years is that things are less and less likely to be open source. You might have an API, but you aren't necessarily going to get the weights and the architecture for something like GPT-4L. So for the rest of the lecture, these are the signposting I'm going to use. Looking back, this is what we expect you've seen before. The green looking forward, that's what we're going to cover in this class. All right. Pretty straightforward. I expect you have seen gradient descent before you come to this class. So gradient descent essentially is the idea that we're trying to optimize some cost function, and the way that we're going to do that is we're finding some minimum over a set of all of our training data, some minimum set of parameters where then the loss is actually minimized for all of those different data points. So what this actually looks like in practice, you're trying to find some theta star, which is like your optimal set of model parameters that minimize your cost function. And so maybe you start somewhere random, and then you calculate your gradient. Now you update the weights of that model in the direction of the gradient, and you step through, et cetera, et cetera. And I think many of you have probably seen this before and will talk in an upcoming lecture, cover some of the different dimensions of things like stochastic gradient descent, et cetera. But we're assuming that you've seen this idea before, probably an intro to ML lecture, for example. Covering in this class, we'll go into a little bit more detail, back prop, and specifically this idea of differential programming. So what does it actually look like to build programming languages that are structured around the idea of optimality and optimizing through gradient descent for different components of that architecture? And I think one of the things that's fun is thinking about what are you actually optimizing for and what are you pushing the gradients through to? So here, one iteration of gradient descent is essentially something like this. And we have some learning rate. And all of that, we're going to go into in a lot more detail next Tuesday, where we're going to be talking about where does this actually come through, how do we think about what some of these choices are, and how we actually build out the learning structure for back prop. I do expect that you've seen things like multilayer perceptrons and nonlinearities, things like relues before. So computation on neural network is often some form of vector in, vector out. And this arrow that goes from vector in to vector out is a lot of what we're going to be talking about in this class. What computation does that cover? How does that actually build out? So one cool thing about neural networks is that they often are reusing the same simple computational units over and over again. So there's often a lot of the same kinds of computation that are being repeated and kind of reused. And what these actually look like, these building blocks that are being used over and over again, they're often some combination of a linear layer. So here, and we will be sticking with this type of notation for the rest of the class, what we call Z sub j, so some component of this output representation. In that linear layer, that would be a sum over i of some weight times each of those input components. Basically, pretty simple. And that can also be written essentially this way, where you have some set of weights and then you also incorporate a bias term. And this is a term that does not actually take in any of those input components. It's just a bias. And that can also be written in a vectorized notation, which makes everything much more simple. So again, just with a focus on the notation, that output unit Z sub j is x, that input vector, transpose times W sub j, so the set of weights that correspond to that output component and that bias term. And then what we consider theta, so that's all the parameters of the model, that's the set of all the weight terms and all the bias terms. So what actually makes a neural net a neural net is the fact that it's not just linear combinations of inputs. You also have some sort of nonlinear function that is mapping your outputs. And that's something like G sub z. And in this case, that's something like a pointwise nonlinearity. So that's an important dimension, that these nonlinearities are usually pointwise. So one possible nonlinearity would be this one, right? G sub z would be 1 if z is greater than 0 or 0 otherwise. Is this a good choice of nonlinearity for a neural net? Why not? Yeah, so it's not differentiable, exactly. Why is the fact that it's not differential not good? Yeah? It hinders back propagation. It hinders back propagation. Why? It makes the gradient descent harder. Yeah, so the directionality, right? If you're anywhere on this graph, you don't know which way to go, essentially. The gradient is 0. So you can try to take a gradient descent step. You're not going to move because the gradient is 0, exactly. So this, roughly, is called a perceptron, right? This linear input plus a pointwise nonlinearity. And you actually can, arguably, do linear classification with a perceptron, right? So if you take a really simple version of this, where your input is just two-dimensional, x1, x2, and you have some w1, w2 learned weight terms that you use to combine to get z, which is your hidden unit, and then you map that via a function to y, you then define z as x transpose w plus b. And y is now some function g of z. What this looks like for any set of values will be something like this, where you essentially have almost like a ramp, right? It's a clear and standard gradient across. Why would it look like this? So why is it smooth and reasonably straight? I maybe didn't pose that question super clearly. So essentially, the reason that mapping it looks like this is because it's a plane, right? So any sort of two-dimensional version of this is going to be a plane, right, with some orientation. And then it's very straightforward, actually, to turn that into a linear classifier, because all you're doing is taking some threshold on that plane. So even a single-layer neural network can perform linear classification, right? Because now this threshold, that's basically what that stepwise nonlinearity is giving you. It's not something that is thresholded at some value. And so even with something quite simple, you can do classification. Now that's assuming that your data is linearly separable, right? So if you have training data like this, which is linearly separable, and now you want to find some optimal set of weights and biases that will minimize some loss term, where now, again, this loss is the difference between the true value. So here, the true value is 1 or 0, and the functional mapping of the inputs to something like that value. If you have something like this, this is what we'd call a bad fit, right? You have seven misclassifications. You're actually doing, you're wrong about more things than you're right. And all of those sort of red values, those are going to give you a high value in your loss function. Or you could find something that's sort of like an OK fit, less misclassifications. Maybe this is something that your model updates to after a single gradient step, right? Your decision service is linear, and now you're sort of fiddling around with what that classifier is, basically which values of weights and biases you need. And then maybe after a few steps, you would find something that is a good fit, where you're not actually misclassifying anything. So it is possible to do classification of linear things, linearly separable things, even with something like a perceptron that's doing these really simple thresholding-based nonlinearities. But like we said, this actually would be really difficult to learn, right? So instead, maybe you would try something like a tanch. So this is another pointwise nonlinearity. And it was one of the sort of earliest ones that people looked at. The things that are nice about this is it's bounded between minus 1 and 1, so none of the values are super huge. And you get saturation of the gradient for super large or super small inputs. You're not getting sort of explosions necessarily. But also, if you start out somewhere that's really far from the center, you don't have a lot of training signal, right? Because we're getting close to 0 in our gradient. And so those gradients do go to 0 as we go to infinity in either direction. The outputs are centered at 0. So the tanh of z can also be written as 2 times the sigmoid of 2z minus 1. So let's look at what that sigmoid function looks like. So in this case, this is a sigmoid, 1 over 1 plus e to the minus h. And e to the minus... Notation is wrong. It should be minus z. This can be interpreted as the firing rate of a neuron. It was kind of the one interpretation of it initially. It's bounded between 0 and 1, so it's not ever negative. Again, you have saturation of that gradient for super large or super small inputs. And all those gradients go to 0. The outputs themselves are centered around 0.5. So it's kind of poor conditioning. It's slightly biased. In practice, we don't actually use this. So instead, in practice, what we often use is something called a rectified linear unit or a ReLU. And this is just the max of 0 and z. And this is unbounded on the positive side, right? So it can go all the way to infinity, which can result in, for example, sometimes things like exploding gradients because the values can get very large. But it's super efficient to implement. Essentially, the derivatives are just that subfunction. And it also seems to help speed up convergence. So in that Krzyzewski paper, they showed that they got something like a 6x speedup using a ReLU over using something like a tanh in terms of converging that model and learning. The drawback is that if you're strongly in the negative region, the unit's what we call dead. There is no gradient. And so you essentially have this gradient collapse where you can no longer learn from that sort of component of the vector. But this is our default choice. And it's widely used in current models. There's lots of sort of slight tweaks to this. But this is really, really common. OK. So then the last thing I want to talk about in this sort of section is stacking layers. So we talked about sort of a single version of this where you have your linear projection. And then you have a non-linearity. But now you can take that and you can just stack another one on top of it. So now both z and h are kind of hidden units. They're somewhere in the middle of the model. We don't necessarily, they're not seen at the inputs or the outputs. And essentially, then you can also kind of remember that all of these things are kind of stacked on top of each other, almost in parallel as well, right? Because all of these inputs are mapping to the different components of the output vector. And so you have weights for all of those different dimensions. So this means that h, sort of our output of the first layer, is going to be g, that pointwise non-linearity, applied to now instead of vectorized w transpose x of j to give us sort of the individual jth component, you can actually just think of this now as a matrix multiplied by a vector, right? All of the weights for all of those different dimensions multiplied by the vector x plus the vector of all your biases b1. So now you have this kind of cleaner notation. And then now y is again g, that same pointwise non-linearity applied on top of your sort of outputs of your first thing. And now with h, right, multiplied by that vector of all your second layer weights and added with your second layer bias term. And so now your set theta, your model representation, is the set of all of the different weights from all the different layers as well as all the different biases in all the different layers. Oh, there's a, thank you. Oh yeah, thank you. So you're asking, in practice, are there any best practices for choosing the non-linearity based on different types of data? I think in the literature, there's maybe sort of, you can kind of see it coming out. But often, people really do use ReLU quite a lot. But I mean, I wouldn't say that there's any sort of great general rules of thumb there. And often, yeah, I would say there's like different things that people have converged upon based on experimental success, right? So you'll sort of see in the literature like, oh yeah, everything in this line of work on these data sets with these things, they're always using one or the other component. And then people kind of build on that. Sometimes that's called grad student gradient descent because you're essentially writing research papers and learning from those research papers what parameters in terms of things like your activation functions make sense. But yeah, I don't know that I have great rules of thumb of like, oh, for this type of application, you should always use this or that. Do you? Yeah, it's not a hard science. Yeah. It's usually about what makes the network more trainable rather than like trying to model a certain kind of data. There are a couple of examples of like using a sinusoidal nonlinearity to pick out some free of people. Like there are a few examples of it, but usually not. And normally, if anyone does deep learning, has a great answer to that question. Yeah. So what Jeremy was saying is basically, it's not a hard science. We don't have like sort of theoretical guarantees of like this is what you should do. But in practice, there are, yeah, there's sort of counter examples to that. So if you know, for example, that your features are sinusoidal, it maybe makes sense to try to pick out like Fourier representations. You're working in the Fourier space, then maybe you should use a sinusoidal activation. Anyway, but yes, I wish that we better understood it. And that's, I think, why we need a lot more research on the theoretical side of like understanding, how do we actually choose these things appropriately instead of maybe learning everything from scratch or doing it experimentally, burning all those trees. Cool. So the parameters of our model are now all the different layers of weights and all the different layers of biases. And so like we said, you can do linear classification with a perceptron. But the fun thing is, oops, you can do nonlinear classification with a deep net, even if that deep net is just a two-layer perceptron. So here, now we've built sort of a double-layer model still only in two dimensions. So now we have mapped through essentially just this simple nonlinearity in between these two different layers. Now, if we look at what this looks like, that first layer is giving us one kind of ramp. The second layer gives us a different ramp because it's a different linear model. And now if you do this nonlinear combination of them, essentially what you get out is you can actually map almost like it's almost like think about the intersection of one ramp with another ramp. You get like a kind of triangular, almost like a pyramidal component that will be higher than everything else. And so now you can still take a simple threshold of that. And now you can get out a nonlinear classification. And of course, you can expand this into many more dimensions. This is, I think, some nice simple intuition building as to why you start getting nonlinear models when you're taking linear combinations but with these nonlinearities. Cool. So another, so we kind of expect that you've seen a lot of that before. But moving forward, one of the things we are going to cover in this class is what we can approximate. So what do we know about what is possible to approximate with deep learning? So one dimension of that is representational power. So you have a single layer. You can do any linear decision surface. If you have two plus layers, in theory, you can actually represent any function. And this is assuming, of course, that you have some non-trivial nonlinearity between those layers. Because if you don't have that nonlinearity, it turns out that a linear layer and another linear layer, this is just a linear combination. It's still linear. So you need that nonlinearity. But doing that, you can actually approximate anything. And one way to think about it is given some amount of capacity, some number of different dimensions that you can do, you can actually approximate any function. This is kind of a rough, hand-wavy intuition thing. But as you're shrinking the size of these different vertical things, we're doing something like a Riemann sum. You can approximate any integral as long as you have enough capacity in terms of the number of small things that you're building up. So this gets at basically one of the issues, which is efficiency. So in theory, you can approximate any complicated function with a sufficiently wide two-layer network. So this means not two dimensions, maybe thousands or millions of dimensions. But in practice, that's actually very inefficient. So a narrow, deep model may be something that only has much fewer dimensions in terms of the size of each vector or each input and output. But now, many, many, many more stacked layers, you might be able to, with a lot less parameters, approximate the same complex function. In practice, we do find that's true. More layers helps. And we'll get into this a little bit in lecture three, which is about approximation theory, the theory of what it is possible to approximate. And then the other thing that we're going to cover in this class is architectures. So we talk about deep networks. Really often, we're talking about something that's roughly like this, some cascade of repeated simple computations, each of these linear and then linear models with a non-linearity. And then with every increasing layer, we're getting more abstracted representations. You're getting a higher capacity model. And this means it can form more complex and abstract calculations, whereas a shallow network, if it's not sufficiently wide, could maybe only efficiently compute simple things, like maybe finding edges. A deep network could efficiently compute harder things, like recognizing this clownfish or translating English to Chinese. And so this classification of complex things, essentially, your whole function f of x is now this sort of stacked or almost like chain-like mapping from your final layer, which is then a functional mapping of your previous layer, and in, and in, and in, and in, all the way back to that input. And there's a lot of different ways that we've learned to build these mappings for different types of problems and based on the structure of the data we're trying to model. And we'll get into that. We'll talk about CNNs, GNNs, transformers, and RNNs. So convolutional neural networks, graph neural networks, transformers, and recurrent neural networks. And then the other thing that we're going to be talking about in this class is when and why we can generalize. So why do deep nets generalize? Essentially, deep nets have so many parameters, they could just act like lookup tables. You just regurgitate the training data. But instead, they seem to learn rules that generalize. And this actually defies classical theory. So the classical theory basically says that if your model is over-parameterized, it will overfit. So there's this capacity versus risk or error curve where if you don't have enough capacity, you're going to underfit the model. And then with too much capacity, you're going to overfit. But in practice, this is from this paper Double Descent, what we see is that in this modern interpolating regime, we go past that point of being over-parameterized. And we're actually then able to actually pass some interpolation threshold or ability to have enough capacity in the model to interpolate between the data points. We're actually able to then potentially get even better, even though the models are massively over-parameterized. Yeah? What do you mean by capacity? Capacity is the number of parameters in the model. So that's both the width and the depth. Basically how many values are captured in those model weights. Yeah. Yeah. Yes, thank you. And how does that relate to the size of the data samples that you're collecting? Yeah. So that is not a dimension on this plot, but it definitely is also related. So if you have, but it's related in a different dimension. If you're learning from fewer data points, it's very easy to learn to maybe overfit to those data points. So you could kind of think of that, honestly, in another way maybe as being a different dimension of capacity here, where now it's like almost the capacity of your training data, though of course there's not like a direct translation. But so if you don't have enough training data, you also can't learn to interpolate between those data points because you don't have enough coverage of the data distribution. I live in that world a lot. We often don't have enough data to kind of learn a general representation without a lot of handy tricks. And we will talk about some of those different tricks in this class, some of the different ways that we try to convince models not to overfit to data and try to learn to sort of generalize beyond maybe what is kind of captured in the data. But I think that dimension is really important. It's one of the reasons that none of this was possible until we started building big enough data sets where you sort of had enough coverage of the space that you wanted to model in your data to be able to start interpolating between them. Yeah. Can you clarify the difference between overfitting and overparameterized? Oh, yeah. So I mean, I think this paper would be great to read to be able to really get into it. But essentially, the idea here is that in the previous literature, we sort of said that at some point, if you had kind of too many parameters, you were going to overfit and learn a model that actually generalized worse. Because based on the amount of data that you had, it was going to kind of, there's like a classical example where you have like three data points. And if you try to learn like a 12-dimensional function, it will learn something like this to fit those data points perfectly. But actually, it's not likely to generalize because it's basically too spiky or too peaky. So that's the idea of overfitting. But this idea of like overparameterized is basically instead of saying underfitting and overfitting, now we're saying that the new paradigm is underparameterized and overparameterized. And what they're basically saying is that you maybe can't be too overparameterized. Though this perspective is one that doesn't take into account resources. So there's many, many people in places and places that are at MIT, but even actually in this class where we don't have infinite compute resources. So actually, what you really want is some optimal point on this curve where you get good performance, but you're not maybe using resources that you don't have, like super, super large memory GPUs. Yeah? When you gave the example of using two very wide layers versus multiple narrow ones, has any of this evolved about which is better for about like interpretability between the two? Yeah. OK. We're definitely going to get into this a lot more in some of the later lectures, kind of like these choices between breadth versus depth in bottles. But at a very high level, your intuition I think that you're getting at is kind of correct. If you have only two layers, actually, maybe not. Because if you have two almost infinite parameter layers, really, really high dimensional space is hard to interpret. If you have lower dimensional space over many, many different now stacked versions of these layers, it's also hard to interpret. So I would say like in ecology, for example, people often still use just generative additive models of a bunch of different linear things because they need the interpretability or they feel like they do. And any times you get to large numbers of parameters in either dimension, that interpretability gets more difficult. I was just going to add that even in like, let's say, open AI, it's like a closely guarded secret like what recipe of like width versus depth that they use in like GPT whatever. If you can come up with a really scientific way to answer that question, you could go and get a job there and like. Make seven figures? Yeah. So what Jeremy is saying is like there's no perfect prescription or recipe for width versus depth and what's optimal. Yeah. So the simplicity hypothesis in classical theory basically says that big models learn complicated functions and overfit the data. And the immersion theory is that deep networks actually learn simple functions that generalize. And we're going to talk about this a lot more in a theoretical and a more experimental lecture around generalization, both in and out of distribution. OK. So again, what I've expected you to see before, softmax cross-entropy loss. Basically, we have this model that we're trying to use to learn to classify something like clownfish. And we assume that you've seen some of these really basic loss functions that we use for this. So for example, if you have this as your last layer and now you have some set of categories you're trying to learn to categorize over, the simple version of this is you take the argmax and you say, OK, well, the thing that's darkest on here is clownfish. And then the loss that you would calculate is, OK, the actual correct ground truth label is clownfish. And then the error between those two, in this case, is 0. So it's small. So you've gotten the correct prediction. And then maybe actually the real ground truth label is grizzly bear. And you've said clownfish. Now what you'd want is for your loss to be large. So you want some loss function that does a good job of punishing you when you're wrong and not punishing you when you're right. And the way we actually do this often is if you assume the net output is some normalized vector, then you can kind of think of this as a probability distribution over different possible object classes. And so what you want, this loss function on the right, this is called cross-entropy loss. This essentially indicates the distance between what the model believes the output distribution should be and what the input distribution actually is, which in this case is what we call a one-hot encoding or essentially a vector that's 0 everywhere except for the place where it's correct, where it's 1. And so essentially, it's just telling the network to maximize the probability of the training data, which forces the output to be a reasonably good probability model of the object class given the image, assuming you have enough training data. So what this looks like in practice, you say clownfish. The ground truth label is clownfish. Maybe this is your prediction. So now the score is basically saying, this is how much better you could have done, how much better you could have done, which is the difference between the amount of probability you gave the class clownfish. I'm putting probability in quotes here. It's super important to know that these are not true probabilities. They're scores. And then that red is kind of the dimension that you could do better, and that's telling you the direction of the gradient you want to go in. So maybe grizzly bear, you got it correct. It's grizzly bear. Now the amount that's left over is small versus actually if it's chameleon and you're saying iguana, essentially this thing is normalized. So if you put a lot more probability on one thing than the correct answer, then you end up with a much smaller amount than how you're looking at the difference between it. The idea with all of this in deep learning is you're taking all these different weights in your massive model, and then you're fiddling around with them until you match the desired output for all your training data. You're basically just wiggling around all these different weights. So you're going through your training data, you're predicting something, you're calculating a loss, and you're doing this for many different data samples. And you're doing it over and over and over again until the model gets everything right or as close to everything right as possible. We also expect that you've seen some amount of parallel processing and the idea of tensors before this. So this is the idea that a lot of this stuff, because it's repeated calculations, you can be done in batch. So each of these individual losses for the individual data points, well, it's all going to be summed anyway. So you can do it in parallel, and now you can take these batches and more efficiently calculate that sum. And so look, you take the same layer for all three of these, you can stack them, and now essentially you get this matrix that's featured, it's multiplied by images. And so that's what your tensors are. They're these multidimensional arrays, and every single layer is some representation of the input data. And actually, everything is a tensor. A tensor is just a multidimensional matrix. So you can say, OK, here's our mathematical representation of this model. You can also just represent it this way. You have these vectors of your different input values, and now you have your batch dimension, which is now this vertical stacking of all those layers. And now you multiply that by your weights matrix, and you get out your output. And then you run that through a pointwise non-linearity. And then again, just another matrix multiplication, and then you get your output z2, and then you take that non-linearity, you get y. So this is why the fact that GPUs can do so many multiplications in parallel was important. Because we can rewrite all of these models as just essentially sets of multiplications. And we'll go into this in a lot more detail as well in the future. So what we're going to cover in this class is how deep networks represent data. So essentially, it's kind of this idea, where deep networks are a more compact way of representing knowledge in data. And previous methods we saw were kind of more like lookup tables. But there's other and maybe better ways to learn a mapping between input and output. Basically assume that there are kind of these lower level building blocks that are useful for many different possible downstream tasks. So one nice example is the idea of trying to learn a classifier for the letter T. So if you want to learn to classify the letter T, you could try to build a classifier just for the letter T. Or you could take a classifier that classifies lines. And you could say, OK, here's a line. And then you could have a rotation of that classifier. And you'd say, here's a line. And now all you have to do is understand the connection point between these two lines. So that line classifier is reusable in some combination to get a T classifier. And that's kind of the basic idea here. And actually, in this post-hoc way, we've analyzed what is learned in the layers of things like convolutional neural networks. And we do see that you'll get low level representations in the earlier layers. Then they get combined into more and more complex representations in those later layers. And you can actually kind of see this as well. If you take an input image and you take those and you sort of cluster them in some low level representation of the space for each layer, the layers earlier on in the network are not going to cluster well by category because they're really just categorizing things like does it have directional orientations of light and dark gradients in the pixels. But actually, where you get towards the end of the network, it's learning more concepts. And that's where you start getting these clusters of different things like fish. And we will sort of talk about actually how that representation learns and is sort of structured through space in a bunch of different lectures on representation learning where we talk about reconstruction, similarity, and theory. We're also going to cover generative models going forward in the class. And this is things like text-based and image-based generation. And we'll cover basics, representations, and conditional models. And then we're going to cover in this class how to think about weight reuse and some of the efficiencies in terms of training if you're able to reuse components of previously trained models. So here, again, this idea that like, all right, well, you learned this thing to categorize pictures of animals. And now maybe you want to categorize some sort of satellite imagery. Well, actually, a lot of those initial components about lines and orientations and structures, those are useful for both of these. So what do we really need to learn from scratch versus what is kind of generally useful in terms of the representations that are learned? And this is really valuable if you don't have big data or big compute, if you can kind of pre-generate representations that then you can learn on top of without needing to learn everything from scratch. And so we'll get into whether or not these things are generalizable or transferable. In these transfer learning lectures, we talk about models and data. And then finally, we'll talk about scaling. So when you think about the scale of a brain, something like this little worm only has 302 neurons. So you can think of that as 302 different sort of values in its neural network. Fruit fly is 15,000. Human beings have 100 billion, roughly. Elephants have 250 billion neurons in their brains. I work with elephants a lot. They're amazing. And we'll talk about scale and deep learning and kind of what this means and actually get into some of the really great questions that different people asked in terms of how does maybe scaling rules change when you're thinking about optimization? What are the laws of scaling? How do we think about something like automatically learning how to best optimize a model? Great. So tried to cover today how we got where we are today. Basically just honestly mostly joking, but definitely like a brief history of sort of these fluctuation and hype cycles. I would say that everyone's pretty clear that we're currently in like a really hypey part of the hype cycle. What we expected you maybe saw before and also a very high level, some of the things we're going to cover in this class. So I'm happy to, if anyone has any final questions. Yeah. How long it'll take the first lecture to get how many views? Well I mean honestly the intro lecture is probably going to be the least watched of all the lectures because it's just a bunch of course logistics. We might even edit those out. But great question. I have no idea."}